#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Gradient compuations
\end_layout

\begin_layout Section
Optimization problem
\end_layout

\begin_layout Standard
Let us consider the following optimization problem
\begin_inset Formula 
\begin{align*}
\arg\min_{A\geq0,P\geq0} & \mathcal{L}(A,P)\\
\text{where} & \mathcal{L}(A,P)=-\langle X,\log(GPA)\rangle+\langle\mathbf{1}^{\ell,p},GPA\rangle
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Gradient computations
\end_layout

\begin_layout Standard
Let us first notice that for
\begin_inset Formula 
\[
f(\Lambda)=\sum_{i,j}X_{ij}\log\left(\Lambda_{ij}\right)=\langle X,\log(\Lambda)\rangle
\]

\end_inset


\begin_inset Formula 
\[
\frac{\partial f}{\partial\Lambda_{i,j}}=\frac{X_{ij}}{\Lambda_{ij}}\hspace{1em}\nabla_{\Lambda}f_{2}=X\oslash\Lambda\neq X\Lambda^{-1}
\]

\end_inset

Here the symbol 
\begin_inset Formula $\oslash$
\end_inset

 means the elementwise division.
 So finally we have:
\begin_inset Formula 
\[
\mathcal{L}(A,P)=-\langle X,\log(GPA)\rangle+\langle\mathbf{1}^{\ell,p},GPA\rangle
\]

\end_inset


\begin_inset Formula 
\[
\nabla_{A}\mathcal{L}=-P^{T}G^{T}\left(X\oslash GPA\right)+P^{T}G^{T}\mathbf{\mathbf{1}^{\ell,p}}
\]

\end_inset


\begin_inset Formula 
\[
\nabla_{P}\mathcal{L}=-G^{T}\left(X\oslash GPA\right)A^{T}+G^{T}\mathbf{1}^{\ell,p}A^{T}
\]

\end_inset

Note that if we add a small term 
\begin_inset Formula $\mathbb{\eta}$
\end_inset

 in the loss: 
\begin_inset Formula $\mathcal{L}^{\prime}(A,P)=-\langle X,\log(GPA+\mathbb{\eta})\rangle+\langle\mathbf{1}^{\ell,p},GPA\rangle$
\end_inset

, we obtain
\begin_inset Formula 
\[
\nabla_{A}\mathcal{L^{\prime}}=-P^{T}G^{T}\left(X\oslash\left(GPA+\mathbb{\eta}\right)\right)+P^{T}G^{T}\mathbf{1}^{\ell,p}
\]

\end_inset


\begin_inset Formula 
\[
\nabla_{P}\mathcal{L}^{\prime}=-G^{T}\left(X\oslash\left(GPA+\mathbb{\eta}\right)\right)A^{T}+G^{T}\mathbf{1}^{\ell,p}A^{T}
\]

\end_inset


\end_layout

\begin_layout Subsection
KKT conditions
\end_layout

\begin_layout Standard
The lagragian is given by
\begin_inset Formula 
\[
L(A,P,\mu)=-\langle X,\log(GPA)\rangle+\langle\mathbf{1}^{\ell,p},GPA\rangle+\mu\circ\left(-A\right)
\]

\end_inset

The KKT conditions for 
\begin_inset Formula $A$
\end_inset

 are
\begin_inset Formula 
\[
\nabla_{A}L(A,P,\mu)=\nabla_{A}\mathcal{L}(A,P)-\mu=0
\]

\end_inset


\begin_inset Formula 
\[
-A\leq\mathbf{0}^{k\times p}
\]

\end_inset


\begin_inset Formula 
\[
\mu\geq\mathbf{0}^{k\times p}
\]

\end_inset


\begin_inset Formula 
\[
A\circ\mu=\mathbf{0}^{k\times p}
\]

\end_inset

Using 
\begin_inset Formula $\nabla_{A}\mathcal{L}(A,P)=\mu,$
\end_inset

 we can rewrite these equations as 
\begin_inset Formula 
\[
\nabla_{A}\mathcal{L}(A,P)\geq\mathbf{0}^{k\times p}
\]

\end_inset


\begin_inset Formula 
\[
A\geq\mathbf{0}^{k\times p}
\]

\end_inset


\begin_inset Formula 
\[
\nabla_{A}\mathcal{L}(A,P)\circ A=\mathbf{0}^{k\times p}
\]

\end_inset

or simply
\begin_inset Formula 
\[
\min\left(\nabla_{A}\mathcal{L}(A,P),A\right)=\mathbf{0}^{k\times p}
\]

\end_inset


\end_layout

\begin_layout Subsection
Link with divergence
\end_layout

\begin_layout Standard
Let us consider the generalized KL divergence
\begin_inset Formula 
\[
D_{GKL}(B\|A)=\sum_{ij}B_{ij}\log\left(\frac{B_{ij}}{A_{ij}}\right)-B_{ij}+A_{ij}
\]

\end_inset

where we have by convention, we have 
\begin_inset Formula $\frac{0}{0}=0$
\end_inset

 and 
\begin_inset Formula $0\log0=0.$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
D\left(X\|GPA\right) & =\sum_{i,j}X_{ij}\log\left(\frac{X_{ij}}{\left[GPA\right]_{ij}}\right)-X_{ij}+\left[GPA\right]_{ij}\\
 & =\sum_{ij}X_{ij}\log\left(X_{ij}\right)-X_{ij}\log\left(\left[GPA\right]_{ij}\right)-X_{ij}+\left[GPA\right]_{ij}
\end{align*}

\end_inset

We observe that if we minimize 
\begin_inset Formula $D\left(X\|GPA\right)$
\end_inset

 with respect of 
\begin_inset Formula $P$
\end_inset

 and 
\begin_inset Formula $A,$
\end_inset

 we recover the loss: 
\begin_inset Formula $\sum_{ij}-X_{ij}\log\left(\left[GPA\right]_{ij}\right)+\left[GPA\right]_{ij}$
\end_inset

 by droping the terms that do not depend of 
\begin_inset Formula $P,A.$
\end_inset

 
\end_layout

\begin_layout Subsection
Toward an iterative algorithm
\end_layout

\begin_layout Standard
Let us consider a gradient step of 
\begin_inset Formula $A$
\end_inset

.
 We have 
\begin_inset Formula 
\begin{align*}
A_{t+1} & =A_{t}-\gamma\circ\nabla_{A}\mathcal{L}\vert_{A_{t},P_{t}}\\
 & =A_{t}-\gamma\circ\left(-P_{t}^{T}G_{t}^{T}\left(X\oslash G_{t}P_{t}A_{t}\right)+P_{t}^{T}G_{t}^{T}\mathbf{\mathbf{1}^{\ell,p}}\right)\\
 & =\left(A_{t}\oslash P_{t}^{T}G_{t}^{T}\mathbf{\mathbf{1}^{\ell,p}}\right)\circ\left(P_{t}^{T}G_{t}^{T}\left(X\oslash G_{t}P_{t}A_{t}\right)\right)
\end{align*}

\end_inset

where we used 
\begin_inset Formula $\gamma=A_{t}\oslash P_{t}^{T}G_{t}^{T}\mathbf{\mathbf{1}^{\ell,p}}$
\end_inset

.
 Similarly, the gradient step for 
\begin_inset Formula $P$
\end_inset

 is
\begin_inset Formula 
\begin{align*}
P_{t+1} & =P_{t}-\gamma\circ\nabla_{P}\mathcal{L}\vert_{A_{t},P_{t}}\\
 & =P_{t}-\gamma\circ\left(-G^{T}\left(X\oslash GPA\right)A^{T}+G^{T}\mathbf{1}^{\ell,p}A^{T}\right)\\
 & =\left(P_{t}\oslash G^{T}\mathbf{1}^{\ell,p}A^{T}\right)\circ\left(G^{T}\left(X\oslash GPA\right)A^{T}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Adding the constraints and regularizers
\end_layout

\begin_layout Standard
First we need to add the constraint that
\begin_inset Formula $A\mathbf{1}^{p}=\mathbf{1}^{k}$
\end_inset

.
 This force 
\begin_inset Formula $A$
\end_inset

 to have the correct normalization.
 Then we add the regularizer 
\begin_inset Formula $m^{T}\log(A)\mathbf{1}^{p}$
\end_inset

, where 
\begin_inset Formula $m$
\end_inset

 is a mask that select all the line of 
\begin_inset Formula $A$
\end_inset

 except the first, i.e.
 
\begin_inset Formula $m^{T}=[0,1,1,\dots,1]$
\end_inset

.
 The problem becomes
\begin_inset Formula 
\begin{align*}
\arg\min_{A\geq0,P\geq0,A\mathbf{1}^{P}=\mathbf{1}^{k}} & \mathcal{L}(A,P)\\
\text{where} & \mathcal{L}(A,P)=-\langle X,\log(GPA)\rangle+\langle\mathbf{1}^{\ell,p},GPA\rangle+\mu M\log(A+\epsilon)\mathbf{1}^{p}
\end{align*}

\end_inset

Let us write the lagrangian 
\begin_inset Formula 
\[
L(A,P,\lambda,\omega,\nu)=-\langle X,\log(GPA)\rangle+\langle\mathbf{1}^{\ell,p},GPA\rangle-\mu m^{T}\log(A+\epsilon)\mathbf{1}^{p}-\langle\lambda,A\rangle-\langle\omega,P\rangle+\nu^{T}\left(A\mathbf{1}^{p}-\mathbf{1}^{k}\right)
\]

\end_inset

Let us leave the positivity constraint for now and we have
\begin_inset Formula 
\[
\tilde{\mathcal{L}}(A,P,\nu)=-\langle X,\log(GPA)\rangle+\langle\mathbf{1}^{\ell,p},GPA\rangle+\mu m^{T}\log(A+\epsilon)\mathbf{1}^{p}+\nu^{T}\left(A\mathbf{1}^{p}-\mathbf{1}^{k}\right)
\]

\end_inset

Using 
\series bold

\begin_inset Formula $GP=D^{\prime},$
\end_inset


\series default
 we rewrite this 
\begin_inset Formula $\tilde{\mathcal{L}}(A,P,\nu)$
\end_inset

 as
\begin_inset Formula 
\[
\tilde{\mathcal{L}}(A,P,\nu)={\color{purple}-\sum_{\ell,p}X_{\ell,p}\log\left(\sum_{k}D_{\ell,k}^{\prime}A_{k,p}\right)}+\sum_{\ell,k,p}D_{\ell,k}^{\prime}A_{k,p}+\mu\sum_{k,p}m_{k}\log(A_{k,p}+\epsilon)+\sum_{k}\nu_{k}\left(\sum_{p}A_{kp}-1\right)
\]

\end_inset

Inspired by 
\backslash
cite{}, we define an auxiliary function
\begin_inset Formula 
\begin{align*}
G(A,A^{t}) & =\sum_{\ell,k,p}D_{\ell,k}^{\prime}A_{k,p}+\mu\sum_{k,p}m_{k}\log\left(A_{k,p}+\epsilon\right)+\sum_{k}\nu_{k}\left(\sum_{p}A_{kp}-1\right)\\
 & {\color{purple}-\sum_{\ell,i,p}X_{\ell,p}\frac{D_{\ell,i}^{\prime}A_{i,p}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}^{t}}\left(\log\left(D_{\ell,i}^{\prime}A_{i,p}\right)-\log\left(\frac{D_{\ell,i}^{\prime}A_{i,p}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}^{t}}\right)\right)}
\end{align*}

\end_inset

We make two observations.
 First 
\begin_inset Formula $\tilde{\mathcal{L}}(A,P,\nu)=G(A,A),$
\end_inset

 as 
\begin_inset Formula 
\begin{align*}
\text{purpleG}(A,A)= & -\sum_{\ell,i,p}X_{\ell,p}\frac{D_{\ell,i}^{\prime}A_{i,p}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}}\left(\log\left(D_{\ell,i}^{\prime}A_{i,p}\right)-\log\left(\frac{D_{\ell,i}^{\prime}A_{i,p}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}}\right)\right)\\
= & -\sum_{\ell,i,p}X_{\ell,p}\frac{D_{\ell,i}^{\prime}A_{i,p}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}}\log\left(\sum_{k}D_{\ell,k}^{\prime}A_{k,p}\right)\\
= & -\sum_{\ell,p}X_{\ell,p}\log\left(\sum_{k}D_{\ell,k}^{\prime}A_{k,p}\right)\sum_{i}\frac{D_{\ell,i}^{\prime}A_{i,p}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}}\\
= & -\sum_{\ell,p}X_{\ell,p}\log\left(\sum_{k}D_{\ell,k}^{\prime}A_{k,p}\right)=\text{purpleL(A)}.
\end{align*}

\end_inset

Second 
\begin_inset Formula $\tilde{\mathcal{L}}(A,P,\nu)\leq G(A,A^{t}),\forall A^{t}$
\end_inset

.
 By convexity of the 
\begin_inset Formula $-\log$
\end_inset

 function, 
\begin_inset Formula 
\begin{align*}
\text{purpleL(A)}= & -\sum_{\ell,p}X_{\ell,p}\log\left(\sum_{i}D_{\ell,i}^{\prime}A_{i,p}\right)\\
\leq & -\sum_{\ell,p}X_{\ell,p}\sum_{i}w_{\ell ip}^{t}\log\left(\frac{D_{\ell,i}^{\prime}A_{i,p}}{w_{\ell ip}^{t}}\right)
\end{align*}

\end_inset

for all non negative 
\begin_inset Formula $w_{\ell ip}^{t}$
\end_inset

 that sum to 
\begin_inset Formula $1$
\end_inset

, i.e.
 
\begin_inset Formula $\sum_{i}w_{\ell ip}^{t}=1$
\end_inset

.
 Let us select 
\begin_inset Formula $w_{\ell ip}^{t}=\frac{D_{\ell,i}^{\prime}A_{i,p}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}^{t}},$
\end_inset

 we have 
\begin_inset Formula 
\[
\text{purpleL(A)}\leq\text{purpleG}(A,A^{t})
\]

\end_inset

This implies that 
\begin_inset Formula $G(A,A^{t})$
\end_inset

 is an auxiliary function of 
\begin_inset Formula $\tilde{\mathcal{L}}(A,P,\nu),$
\end_inset

 see 
\backslash
cite[Definition 1]{}.
 Hence by minimizing 
\begin_inset Formula $G(A,A^{t})$
\end_inset

 with respect of 
\begin_inset Formula $A$
\end_inset

, we obtain an non-increasing update for 
\begin_inset Formula $\tilde{\mathcal{L}}(A,P,\nu)$
\end_inset

, see 
\backslash
cite[Lemma 1]{}.
 
\end_layout

\begin_layout Subsection
Small remark about the auxiliary function
\end_layout

\begin_layout Standard
For simplicity, let us drop the constraint 
\begin_inset Formula $\sum_{k}\nu_{k}\left(\sum_{p}A_{kp}-1\right)$
\end_inset

 and the regularization 
\begin_inset Formula $\mu\sum_{k,p}m_{k}\log\left(A_{k,p}+\epsilon\right)$
\end_inset

 terms.
 We have
\begin_inset Formula 
\begin{align*}
G(A,A^{t}) & =\sum_{\ell,p}\left[-\sum_{i}X_{\ell,p}\frac{D_{\ell,i}^{\prime}A_{i,p}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}^{t}}\left(\log\left(D_{\ell,i}^{\prime}A_{i,p}\right)-\log\left(\frac{D_{\ell,i}^{\prime}A_{i,p}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}^{t}}\right)\right)+\sum_{i}D_{\ell,i}^{\prime}A_{i,p}\right]\\
 & =\sum_{\ell,p}\left[X_{\ell,p}\sum_{i}w_{i}^{t,\ell,k}\log\left(\frac{w_{i}^{t,\ell,k}}{Z_{i}^{\ell,p}}\right)+\sum_{i}Z_{i}^{\ell,p}\right]
\end{align*}

\end_inset

where we have set set 
\begin_inset Formula $w_{i}^{t,\ell,k}=\frac{D_{\ell,i}^{\prime}A_{i,p}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}^{t}}$
\end_inset

 and 
\begin_inset Formula $D_{\ell,i}^{\prime}A_{i,p}=Z_{i}^{\ell,p}$
\end_inset

.
 We have used
\begin_inset Formula 
\begin{align*}
-\sum_{i}Xw_{i}^{t}\left(\log\left(Z_{i}\right)-\log\left(w_{i}^{t}\right)\right) & =X\sum_{i}w_{i}^{t}\log\left(\frac{w_{i}^{t}}{Z_{i}}\right)
\end{align*}

\end_inset

Remember that the definition of the generalized KL divergence satisfies
\begin_inset Formula 
\[
D_{GKL}(B\|A)+\sum_{i}B_{i}-\sum_{i}A_{i}=\sum_{i}B_{i}\log\left(\frac{B_{i}}{A_{i}}\right)
\]

\end_inset

So we can rewrite 
\begin_inset Formula $G$
\end_inset

 as
\begin_inset Formula 
\begin{align*}
G(A,A^{t}) & =\sum_{\ell,p}\left[X_{\ell,p}\sum_{i}w_{i}^{t,\ell,k}\log\left(\frac{w_{i}^{t,\ell,k}}{Z_{i}^{\ell,p}}\right)+\sum_{i}Z_{i}^{\ell,p}\right]\\
 & =\sum_{\ell,p}\left[X_{\ell,p}\left(D_{GKL}(w^{t,\ell,k}\|Z^{\ell,p})+1-\sum_{i}Z_{i}^{\ell,p}\right)+\sum_{i}Z_{i}^{\ell,p}\right]\\
 & =\sum_{\ell,p}\left[X_{\ell,p}D_{GKL}(w^{t,\ell,k}\|Z^{\ell,p})+X_{\ell,p}+(1-X_{\ell,p})\sum_{i}Z_{i}^{\ell,p}\right]
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
G(A,A^{t})=F(A^{t})+\nabla F(A^{t})(A-A^{t})+\frac{1}{\gamma}D_{GKL}(A,A^{t})
\]

\end_inset

I am not sure where I am going.
 We have established that
\begin_inset Formula 
\[
G(A,A^{t})\ge\mathcal{L}(A,D)
\]

\end_inset


\begin_inset Formula 
\begin{align*}
\mathcal{L}(A,D) & =\sum_{\ell,p}\left[-X_{\ell,p}\log\left(\sum_{i}D_{\ell,i}^{\prime}A_{i,p}\right)+\sum_{i}D_{\ell,i}^{\prime}A_{i,p}\right]\\
 & =\sum_{\ell,p}\left[-X_{\ell,p}\log\left(\sum_{i}Z_{i}^{\ell,p}\right)+\sum_{i}Z_{i}^{\ell,p}\right]
\end{align*}

\end_inset

Note that the generalized KL divergence is a particular case of the bregman
 divergence
\begin_inset Formula 
\[
D_{f}(B\|A)=f(B)-\langle\nabla f(A),B-A\rangle-f(A),
\]

\end_inset

 where 
\begin_inset Formula $f(A)=\sum_{i}A_{i}\log(A_{i})=-H(A)$
\end_inset

.
\end_layout

\begin_layout Standard
Let us assume that we have 
\begin_inset Formula $f(A)=\sum_{i}A_{i}\log(A_{i})$
\end_inset

, then 
\begin_inset Formula $\nabla f(A)=\log\left(A\right)+1$
\end_inset


\begin_inset Formula 
\begin{align*}
D_{f}(B\|A) & =f(B)-\langle\nabla f(A),B-A\rangle-f(A)\\
 & =\sum_{i}B_{i}\log(B_{i})-\langle\log\left(A\right)+1,B-A\rangle-\sum_{i}A_{i}\log(A_{i})\\
 & =\sum_{i}\left[B_{i}\log(B_{i})-B_{i}\log\left(A_{i}\right)-B_{i}+A_{i}\right]\\
 & =D_{GKL}(B\|A)
\end{align*}

\end_inset


\begin_inset Formula 
\[
-X_{\ell,p}\log\left(\sum_{i}Z_{i}^{\ell,p}\right)\leq-X_{\ell,p}\sum_{i}w_{\ell ip}^{t}\log\left(\frac{Z_{i}^{\ell,p}}{w_{\ell ip}^{t}}\right)
\]

\end_inset


\end_layout

\begin_layout Subsection
Finding the minimum of the auxiliary function
\end_layout

\begin_layout Standard
Let us search for the 
\begin_inset Formula $A$
\end_inset

 such that the gradient of 
\begin_inset Formula $G(A,A^{t})$
\end_inset

 is 
\begin_inset Formula $0$
\end_inset


\begin_inset Formula 
\begin{align*}
\frac{\partial G(A,A^{t})}{\partial A_{k_{0},p_{0}}} & =\sum_{\ell}D_{\ell,k_{0}}^{\prime}+\mu_{k_{0}}\frac{1}{A_{k_{0},p_{0}}+\epsilon}+\nu_{k_{0}}\\
 & -\frac{1}{A_{k_{0},p_{0}}}\sum_{\ell}X_{\ell,p_{0}}\frac{D_{\ell,k_{0}}^{\prime}A_{k_{0},p_{0}}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p_{0}}^{t}}\\
 & =0
\end{align*}

\end_inset

Let us linearize the log around 
\begin_inset Formula $A_{k_{0},p_{0}}+\epsilon.$
\end_inset

 We have 
\begin_inset Formula 
\[
\log(\epsilon+x)_{x\rightarrow a}\approx\log(\epsilon+a)+\frac{x-a}{\epsilon+a}
\]

\end_inset

which as a derivative of 
\begin_inset Formula $\frac{1}{\epsilon+a}$
\end_inset

 in 
\begin_inset Formula $a$
\end_inset

.
 If we assume that 
\begin_inset Formula $A_{k_{0},p_{0}}^{t}\approx A_{k_{0},p_{0}}$
\end_inset

, which implies we update 
\begin_inset Formula $A$
\end_inset

, slowly, we have for the derivative
\begin_inset Formula 
\begin{align*}
\frac{\partial G(A,A^{t})}{\partial A_{k_{0},p_{0}}} & =\sum_{\ell}D_{\ell,k_{0}}^{\prime}+\mu_{k_{0}}\frac{1}{A_{k_{0},p_{0}}^{t}+\epsilon}+\nu_{k_{0}}\\
 & -\frac{1}{A_{k_{0},p_{0}}}\sum_{\ell}X_{\ell,p_{0}}\frac{D_{\ell,k_{0}}^{\prime}A_{k_{0},p_{0}}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p_{0}}^{t}}=0
\end{align*}

\end_inset

which leads to 
\begin_inset Formula 
\[
A_{k_{0},p_{0}}=A_{k_{0},p_{0}}^{t}\frac{\sum_{\ell}\frac{X_{\ell,p_{0}}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p_{0}}^{t}}D_{\ell,k_{0}}^{\prime}}{\sum_{\ell}D_{\ell,k_{0}}^{\prime}+\nu_{k_{0}}+\frac{\mu_{k_{0}}}{A_{k_{0},p_{0}}^{t}+\epsilon}}
\]

\end_inset

Eventually, we need to find the 
\begin_inset Formula $\nu_{k_{0}}$
\end_inset

 such that 
\begin_inset Formula $\sum_{p_{0}}A_{k_{0},p_{0}}=1.$
\end_inset

 Basically, we need to solve: 
\begin_inset Formula 
\begin{align*}
\sum_{i}A_{i} & =\sum_{i}\frac{a_{i}}{b_{i}+\nu}=1
\end{align*}

\end_inset

Assuming 
\begin_inset Formula $a_{i}>0$
\end_inset

, 
\begin_inset Formula $b_{i}\geq0$
\end_inset

 and 
\begin_inset Formula $\nu\geq0.$
\end_inset

 This can be done using dichotomy as implemented by Adrien.
 
\end_layout

\begin_layout Section
Adding a smooth term
\end_layout

\begin_layout Standard
Let us consider the following objective function
\begin_inset Formula 
\begin{align*}
\mathcal{L}_{S}(A,P) & ={\color{magenta}-\langle X,\log(GPA)\rangle+\langle\mathbf{1}^{\ell,p},GPA\rangle}{\color{green}+\mu M\log(A+\epsilon)\mathbf{1}^{p}}{\color{blue}+\frac{\lambda}{2}\text{tr}\left(A^{T}\Delta A\right)}\\
 & ={\color{magenta}\mathcal{L}(A,P)}{\color{green}+R_{1}(A)}{\color{blue}+R_{2}(A)}
\end{align*}

\end_inset

We create an auxiliary function 
\begin_inset Formula $G$
\end_inset

 such that all these three function are upper bounded.
 
\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula ${\color{magenta}\mathcal{L}(A,P)}$
\end_inset

, we set 
\begin_inset Formula $w_{i}^{t,\ell,k}=\frac{D_{\ell,i}^{\prime}A_{i,p}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}^{t}}$
\end_inset

 and 
\begin_inset Formula $D_{\ell,i}^{\prime}A_{i,p}=Z_{i}^{\ell,p}$
\end_inset

 and obtain
\begin_inset Formula 
\begin{align*}
{\color{magenta}\mathcal{L}(A,P)} & \leq\sum_{\ell,p}\left[X_{\ell,p}\sum_{i}w_{i}^{t,\ell,k}\log\left(\frac{w_{i}^{t,\ell,k}}{Z_{i}^{\ell,p}}\right)+\sum_{i}Z_{i}^{\ell,p}\right]\\
 & =\sum_{\ell,p}\left[-\sum_{i}X_{\ell,p}\frac{D_{\ell,i}^{\prime}A_{i,p}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}^{t}}\left(\log\left(D_{\ell,i}^{\prime}A_{i,p}\right)-\log\left(\frac{D_{\ell,i}^{\prime}A_{i,p}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}^{t}}\right)\right)+\sum_{i}D_{\ell,i}^{\prime}A_{i,p}\right]
\end{align*}

\end_inset

This comes from the fact that
\begin_inset Formula 
\[
-\sum_{\ell,p}X_{\ell,p}\log\left(\sum_{i}Z_{i}^{\ell,p}\right)\leq-\sum_{\ell,p}X_{\ell,p}\sum_{i}w_{\ell ip}^{t}\log\left(\frac{Z_{i}^{\ell,p}}{w_{\ell ip}^{t}}\right)
\]

\end_inset


\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula ${\color{green}R_{1}(A)},$
\end_inset

 we linearize the function around 
\begin_inset Formula $A^{t}$
\end_inset

 and thanks to concavity, we have
\begin_inset Formula 
\[
{\color{green}R_{1}(A)\leq\sum_{k,p}\mu_{k}\left[\log\left(A_{k,p}^{t}+\epsilon\right)+\frac{A-A_{k,p}^{t}}{\epsilon+A_{k,p}^{t}}\right]}
\]

\end_inset


\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula ${\color{blue}R_{2}(A)},$
\end_inset

 we use the key computation presented bellow
\begin_inset Formula 
\begin{align*}
{\color{blue}R_{2}(A)} & =\frac{\lambda}{2}\text{tr}\left(A^{T}\Delta A\right)\\
 & =\frac{\lambda}{2}\sum_{k}A_{k}^{T}\Delta A_{k}\\
 & \leq\frac{\lambda}{2}\sum_{k}A_{k}^{t}{}^{T}\Delta A_{k}^{t}+2\left(A_{k}-A_{k}^{t}\right)^{T}\Delta A_{k}^{t}+2\lambda\sigma(\Delta)\left(\max_{k,p}A_{k,p}^{t}\right)\sum_{p,k}\left(A_{k,p}^{t}\log\left(\frac{A_{k,p}^{t}}{A_{k,p}}\right)-A_{k,p}^{t}+A_{k,p}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
As a result, we create an auxiliary defined as 
\begin_inset Formula 
\begin{align*}
G\left(A,A^{t}\right) & =\sum_{\ell,p}\left[-\sum_{i}X_{\ell,p}\frac{D_{\ell,i}^{\prime}A_{i,p}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}^{t}}\left(\log\left(D_{\ell,i}^{\prime}A_{i,p}\right)-\log\left(\frac{D_{\ell,i}^{\prime}A_{i,p}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}^{t}}\right)\right)+\sum_{i}D_{\ell,i}^{\prime}A_{i,p}\right]\\
 & +\sum_{k,p}\mu_{k}\left[\log\left(A_{k,p}^{t}+\epsilon\right)+\frac{A-A_{k,p}^{t}}{\epsilon+A_{k,p}^{t}}\right]+\sum_{k}\nu_{k}\left(\sum_{p}A_{kp}-1\right)\\
 & +\frac{\lambda}{2}\sum_{i}\left[A_{i}^{t}{}^{T}\Delta A_{i}^{t}+2\left(A_{i}-A_{i}^{t}\right)^{T}\Delta A_{i}^{t}\right]+2\sigma(\Delta)\left(\max_{i,p}A_{i,p}^{t}\right)\sum_{i,p}\left(A_{i,p}^{t}\log\left(\frac{A_{i,p}^{t}}{A_{i,p}}\right)-A_{i,p}^{t}+A_{i,p}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Computation of the auxiliary function minimum
\end_layout

\begin_layout Standard
Let us compute the gradient of the auxiliary function 
\begin_inset Formula $G$
\end_inset

 with respect of 
\begin_inset Formula $A$
\end_inset

 
\begin_inset Formula 
\begin{align*}
\frac{\partial G(A,A^{t})}{\partial A_{k_{0},p_{0}}} & =\sum_{\ell}D_{\ell,k_{0}}^{\prime}-\frac{1}{A_{k_{0},p_{0}}}\sum_{\ell}X_{\ell,p_{0}}\frac{D_{\ell,k_{0}}^{\prime}A_{k_{0},p_{0}}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p_{0}}^{t}}\\
 & +\frac{\mu_{k_{0}}}{\epsilon+A_{k_{0},p_{0}}^{t}}+\nu_{k_{0}}\\
 & +\frac{\lambda}{2}\left[2\sum_{k_{2}}A_{p_{0},k_{2}}^{tT}\Delta_{k_{2}k_{0}}+2\sigma(\Delta)\left(\max_{k,p}A_{k,p}^{t}\right)\left(-\frac{A_{k_{0},p_{0}}^{t}}{A_{k_{0},p_{0}}}+1\right)\right]\\
 & =-\frac{1}{A_{k_{0},p_{0}}}\left(\sum_{\ell}X_{\ell,p_{0}}\frac{D_{\ell,k_{0}}^{\prime}A_{k_{0},p_{0}}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p_{0}}^{t}}+\lambda\sigma(\Delta)\left(\max_{k,p}A_{k,p}^{t}\right)A_{k_{0},p_{0}}^{t}\right)\\
 & +\sum_{\ell}D_{\ell,k_{0}}^{\prime}+\frac{\mu_{k_{0}}}{\epsilon+A_{k_{0},p_{0}}^{t}}+\lambda\sum_{k_{2}}A_{p_{0},k_{2}}^{tT}\Delta_{k_{2}k_{0}}+\lambda\sigma(\Delta)\left(\max_{k,p}A_{k,p}^{t}\right)+\nu_{k_{0}}\\
 & =0
\end{align*}

\end_inset

So we obtain we obtain the following solution
\begin_inset Formula 
\[
A_{k_{0},p_{0}}=A_{k_{0},p_{0}}^{t}\frac{\sum_{\ell}X_{\ell,p_{0}}\frac{D_{\ell,k_{0}}^{\prime}A_{k_{0},p_{0}}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p_{0}}^{t}}+\lambda\sigma(\Delta)\left(\max_{k,p}A_{k,p}^{t}\right)A_{k_{0},p_{0}}^{t}}{\sum_{\ell}D_{\ell,k_{0}}^{\prime}+\frac{\mu_{k_{0}}}{\epsilon+A_{k_{0},p_{0}}^{t}}+\lambda\sum_{k_{2}}A_{p_{0},k_{2}}^{tT}\Delta_{k_{2}k_{0}}+\lambda\sigma(\Delta)\left(\max_{k,p}A_{k,p}^{t}\right)+\nu_{k_{0}}}
\]

\end_inset


\end_layout

\begin_layout Section
Key computation
\end_layout

\begin_layout Standard
In this subsection, we prove that
\begin_inset Formula 
\begin{align*}
p^{T}Lp & \le p_{t}^{T}Lp_{t}+2\left(p-p_{t}\right)^{T}Lp_{t}+2\sigma(L)\left(\max_{i}p_{i}\right)D_{GKL}\left(p_{t}||p\right),\\
 & =p_{t}^{T}Lp_{t}+2\left(p-p_{t}\right)^{T}Lp_{t}+2\sigma(L)\left(\max_{i}p_{i}\right)\left(\sum_{i=1}^{k}p_{i}\log\left(\frac{p_{ti}}{p_{i}}\right)-\sum_{i=1}^{k}p_{ti}+\sum_{i=1}^{k}p_{i}\right)
\end{align*}

\end_inset


\begin_inset Formula $\forall p,p_{t}\in\mathbb{R}^{k}$
\end_inset

, 
\begin_inset Formula $L\in\mathbb{R}^{k\times k}$
\end_inset

 and 
\begin_inset Formula $\sigma(L)=\|L\|_{op}.$
\end_inset

 
\end_layout

\begin_layout Standard
Here is some key computation
\begin_inset Formula 
\[
\|p-q\|^{2}\leq2D_{GKL}\left(p||q\right)
\]

\end_inset

given 
\begin_inset Formula $p,q\in\mathbb{R}^{k}$
\end_inset


\begin_inset Formula 
\[
H(p)=-\sum_{i=1}^{k}p_{i}\log\left(p_{i}\right)
\]

\end_inset

and 
\begin_inset Formula $V(p)=-H(p)$
\end_inset

, 
\begin_inset Formula $\nabla V(p)=\log\left(p\right)+1$
\end_inset

.
 So we have
\begin_inset Formula 
\begin{align*}
D_{GKL}\left(p||q\right) & =\sum_{i=1}^{k}p_{i}\log\left(\frac{p_{i}}{q_{i}}\right)-\sum_{i=1}^{k}p_{i}+\sum_{i=1}^{k}q_{i}\\
 & =\sum_{i=1}^{k}p_{i}\log\left(p_{i}\right)-\sum_{i=1}^{k}p_{i}\log\left(q_{i}\right)-\sum_{i=1}^{k}p_{i}+\sum_{i=1}^{k}q_{i}\\
 & =\sum_{i=1}^{k}p_{i}\log\left(p_{i}\right)-\sum_{i=1}^{k}q_{i}\log\left(q_{i}\right)-\sum_{i}\left[\left(\log\left(q_{i}\right)+1\right)\left(p_{i}-q_{i}\right)\right]\\
 & =D_{V}(p\|q)\\
 & =V(p)-\left(V(q)+\nabla V(q)^{T}(p-q)\right)\\
 & =\frac{1}{2}\left(q-p\right)^{T}\mathcal{H}(\tilde{p})\left(q-p\right)\\
 & \geq\frac{1}{2}\frac{1}{\max_{i}p_{i}}\|p-q\|^{2}
\end{align*}

\end_inset

There exist at least a 
\begin_inset Formula $\tilde{p}$
\end_inset

 such that this is true.
 Similarly, we can show that 
\begin_inset Formula $D_{GKL}\left(q||p\right)\geq\frac{1}{2}\|p-q\|^{2}$
\end_inset

 The last inequality comes from the fact that for 
\begin_inset Formula $V(p)=-H(p),$
\end_inset


\begin_inset Formula 
\[
\mathcal{H}(p)=\left[\frac{\partial^{2}V}{\partial p_{i}\partial p_{j}}\right]_{ij}\geq\frac{1}{\max_{i}p_{i}}I
\]

\end_inset

since
\begin_inset Formula 
\[
\frac{\partial V}{\partial p_{i}}\log\left(p_{i}\right)+1\hspace{1em}\text{and}\hspace{1em}\frac{\partial^{2}V}{\partial p_{i}\partial p_{j}}=\delta_{ij}\frac{1}{p_{i}}\geq\frac{1}{\max_{i}p_{i}}.
\]

\end_inset

Furthermore a tailor developement of 
\begin_inset Formula $p^{T}Lp$
\end_inset

 gives 
\begin_inset Formula 
\begin{align*}
p^{T}Lp & =p_{t}^{T}Lp_{t}+2\left(p-p_{t}\right)^{T}Lp_{t}+\left(p-p_{t}\right)^{T}L\left(p-p_{t}\right)\\
 & \leq p_{t}^{T}Lp_{t}+2\left(p-p_{t}\right)^{T}Lp_{t}+\sigma(L)\|p-p_{t}\|^{2}\\
 & \leq p_{t}^{T}Lp_{t}+2\left(p-p_{t}\right)^{T}Lp_{t}+2\left(\max_{i}p_{i}\right)\sigma(L)D_{GKL}\left(p_{t}||p\right)
\end{align*}

\end_inset

Note that is only true if 
\begin_inset Formula $p\leq1!!!!$
\end_inset

 If 
\begin_inset Formula $p_{t}>1,$
\end_inset

 we can simply renormalize the bound
\end_layout

\begin_layout Section
Two-metrics
\end_layout

\begin_layout Standard
\begin_inset Formula $A$
\end_inset

 should be on the simplex.
 But problematically, the updates get us out of the simplex.
 If you minimize
\begin_inset Formula 
\[
L(p)=D_{KL}\left(p||q\right)+a^{T}q
\]

\end_inset


\begin_inset Formula 
\[
f(a)\leq f(a_{t})+\nabla f(a_{t})^{T}\left(a-a_{t}\right)+\gamma D_{GKL}(a_{t}||a)=\tilde{f}_{t}(a)
\]

\end_inset

such that 
\begin_inset Formula $a$
\end_inset

 is in the simplex.
 Because everything is convex, we can replace
\begin_inset Formula $\nabla f(a_{t})^{T}$
\end_inset

 by its projection.
\end_layout

\begin_layout Section
Alteranative to dichotomy
\end_layout

\begin_layout Standard
Problematically, the dicotomy might not lead the correct root.
 Let us write the function
\begin_inset Formula 
\[
f(\nu)=\sum_{i}\frac{a_{i}}{b_{i}+\nu}-1
\]

\end_inset

If 
\begin_inset Formula $f(0)\geq0,$
\end_inset

 then we would like to find the smallest 
\begin_inset Formula $\nu$
\end_inset

such that Another approach would be to start with 
\begin_inset Formula $\nu$
\end_inset

 eqal 0
\end_layout

\begin_layout Subsection
Alternative solution
\end_layout

\begin_layout Standard
Since the problem is convex both in 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $P$
\end_inset

 (but not jointly), we can solve alternatively
\begin_inset Formula 
\[
A_{t+1}=\arg\min_{A\geq0,A,\mathbf{1}^{P}=\mathbf{1}^{k}}-\langle X,\log(GP_{t}A)\rangle+\langle\mathbf{1}^{\ell,p},GP_{t}A\rangle
\]

\end_inset


\begin_inset Formula 
\[
P_{t+1}=\arg\min_{P\geq0}-\langle X,\log(GPA_{t+1})\rangle+\langle\mathbf{1}^{\ell,p},GPA_{t+1}\rangle
\]

\end_inset


\end_layout

\begin_layout Standard
Let us first compute the proximal operator of the function 
\begin_inset Formula $f(Z)=-\langle X,\log\left(Z\right)\rangle.$
\end_inset

 We have 
\begin_inset Formula 
\[
\text{prox}_{f}\left(Y,\lambda\right)=\arg\min_{Z}\frac{1}{2}\|Z-Y\|_{2}^{2}-\lambda\langle X,\log\left(Z\right)\rangle
\]

\end_inset

To find the solution, we find the point with a zero gradient: 
\begin_inset Formula $Z-Y-\lambda X\oslash Z=\mathbf{0.}$
\end_inset

 Assuming that 
\begin_inset Formula $Z>0,$
\end_inset

 We find the second order equation 
\begin_inset Formula 
\[
Z\circ Z-YZ-\lambda X=\mathbf{0},
\]

\end_inset

 which gives us 
\begin_inset Formula $\frac{1}{2}\left(Y\pm\sqrt{Y\circ Y+4\lambda X}\right).$
\end_inset

 As the solution has to be positive, we have 
\begin_inset Formula 
\[
\text{prox}_{f}\left(Y,\lambda\right)=\frac{1}{2}\left(Y+\sqrt{Y\circ Y+4\lambda X}\right).
\]

\end_inset

Adding the constraints and regularizers
\end_layout

\begin_layout Section
L2 algorithm
\end_layout

\begin_layout Standard
Let us consider the following optimization problem
\begin_inset Formula 
\begin{align*}
\arg\min_{A\geq0,P\geq0} & \mathcal{L}(A,P)\\
\text{where} & \mathcal{L}(A,P)=\frac{1}{2}\|GPA-X\|_{F}^{2}{\color{green}+\mu^{T}\log(A+\epsilon)\mathbf{1}^{p}}{\color{blue}+\frac{\lambda}{2}\text{tr}\left(A^{T}\Delta A\right)+\left(\mathbf{1}^{cT}A-\mathbf{1}^{p}\right)\nu}
\end{align*}

\end_inset

The gradient is given by
\begin_inset Formula 
\begin{align*}
\nabla_{P}\mathcal{L}(A,P) & =G^{T}(GPA-X)A^{T}\\
 & =G^{T}GPAA^{T}-G^{T}XA^{T}
\end{align*}

\end_inset


\begin_inset Formula 
\begin{align*}
\nabla_{A}\mathcal{L}(A,P) & =P^{T}G^{T}(GPA-X)+\mu^{T}\mathbf{1}^{p}\oslash(A+\epsilon)+\lambda\Delta A+\mathbf{1}^{c}\nu^{T}\\
 & =P^{T}G^{T}GPA-P^{T}G^{T}X+\mu^{T}\mathbf{1}^{p}\oslash(A+\epsilon)+\lambda\Delta A
\end{align*}

\end_inset

A gradient step can be done as
\begin_inset Formula 
\begin{align*}
P_{t+1} & =P_{t}-\gamma\nabla_{P}\mathcal{L}(A_{t},P_{t})\\
 & =P_{t}-\gamma G^{T}GP_{t}A_{t}A_{t}^{T}+\gamma G^{T}XA_{t}^{T}\\
 & =P_{t}\oslash G^{T}GP_{t}A_{t}A_{t}^{T}\circ G^{T}XA_{t}^{T}
\end{align*}

\end_inset


\begin_inset Formula 
\begin{align*}
A_{t+1} & =A_{t}-\gamma\nabla_{P}\mathcal{L}(A_{t},P_{t})\\
 & =A_{t}-\gamma\left(P^{T}G^{T}GPA-P^{T}G^{T}X+\mu^{T}\mathbf{1}^{p}\oslash(A+\epsilon)+\lambda\Delta A+\mathbf{1}^{c}\nu^{T}\right)\\
 & =A_{t}\oslash\left(P^{T}G^{T}GPA+\lambda\Delta A+\mu^{T}\mathbf{1}^{p}\oslash(A+\epsilon)+\mathbf{1}^{c}\nu^{T}\right)\circ P^{T}G^{T}X
\end{align*}

\end_inset

where 
\begin_inset Formula $\gamma=P_{t}\oslash G^{T}GP_{t}A_{t}A_{t}^{T}$
\end_inset

 and 
\begin_inset Formula $A_{t}\oslash\left(P^{T}G^{T}GPA+\lambda\Delta A+\mu^{T}\mathbf{1}^{p}\oslash(A+\epsilon)+\mathbf{1}^{c}\nu^{T}\right)$
\end_inset

.
\end_layout

\begin_layout Section
Projected algorithm
\end_layout

\begin_layout Standard
The definition of the Bregman divergence is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D_{f}(B\|A)=f(B)-\langle\nabla f(A),B-A\rangle-f(A)
\]

\end_inset

 If we select 
\begin_inset Formula $f(A)=\sum_{i}A_{i}\log(A_{i})=-H(A)$
\end_inset

, we have 
\begin_inset Formula $\nabla f(A)=\log\left(A\right)+1$
\end_inset

 and
\begin_inset Formula 
\begin{align*}
D_{f}(B\|A) & =f(B)-\langle\nabla f(A),B-A\rangle-f(A)\\
 & =\sum_{i}B_{i}\log(B_{i})-\langle\log\left(A\right)+1,B-A\rangle-\sum_{i}A_{i}\log(A_{i})\\
 & =\sum_{i}\left[B_{i}\log(B_{i})-B_{i}\log\left(A_{i}\right)-B_{i}+A_{i}\right]\\
 & =D_{GKL}(B\|A)
\end{align*}

\end_inset

From the definition, we have
\begin_inset Formula 
\[
f(B)=f(A)+\langle\nabla f(A),B-A\rangle+D_{f}(B\|A)
\]

\end_inset

Let us use this relation in our problem and assume we would like to minimize
 
\begin_inset Formula $-\langle X,\log(GPA)\rangle+\langle\mathbf{1}^{\ell,p},GPA\rangle$
\end_inset

.
 We start with 
\begin_inset Formula $f(A)=-\langle X,\log(GPA)\rangle$
\end_inset

 and find 
\begin_inset Formula 
\begin{align*}
f(A) & =f(A_{t})+\langle\nabla f(A^{t}),A-A^{t}\rangle+D_{f}(A\|A^{t})\\
 & =-\langle X,\log(GPA^{t})\rangle+\langle\nabla f(A^{t}),A-A^{t}\rangle+D_{f}(A\|A^{t})\\
 & =
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Idea from Guillaume
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
f(x) & \leq f(x_{t})+\langle\nabla f(x_{t}),x-x_{t}\rangle+D_{B}(x\|x_{t})\\
 & =f(x_{t})+\langle\Pi\nabla f(x_{t}),x-x_{t}\rangle+D_{B}(x_{t}\|x)
\end{align*}

\end_inset


\begin_inset Formula 
\[
\text{s.t. }x\in S
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $D_{B}$
\end_inset

 is the bregman divergence
\begin_inset Formula 
\[
D_{B}(x_{t}\|x)=-\sum_{i}x_{ti}\log\left(\frac{x_{i}}{x_{it}}\right)
\]

\end_inset


\begin_inset Formula $\Pi:$
\end_inset

 projection on 
\begin_inset Formula $\{x|x^{T}1=0\}$
\end_inset


\begin_inset Formula 
\[
\min_{x_{i}}\sum_{i=1}^{K}g_{ti}x_{i}-\sum
\]

\end_inset


\end_layout

\begin_layout Standard
regle d'Armiro
\end_layout

\begin_layout Subsection
Some tests
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula 
\[
F(A)=-\langle X,\log(DA)\rangle+\langle\mathbf{1}^{\ell,p},DA\rangle
\]

\end_inset

and define 
\begin_inset Formula $G(A,A^{t})$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
G(A,A^{t})=F(A^{t})+\nabla F(A^{t})(A-A^{t})+\frac{1}{\gamma}D_{F}(A\|A^{t})
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla F(A^{t})=-D^{T}\left(X\oslash DA\right)+D^{T}\mathbf{\mathbf{1}}^{\ell,p}
\]

\end_inset


\begin_inset Formula 
\begin{align*}
D_{F}(A\|A^{t}) & =F(A)-\langle\nabla F(A^{t}),A-A^{t}\rangle-F(A^{t})\\
 & =-\langle X,\log(DA)\rangle+\langle\mathbf{1}^{\ell,p},DA\rangle\\
 & +\langle D^{T}\left(X\oslash DA^{t}\right),(A-A^{t})\rangle-\langle D^{T}\mathbf{\mathbf{1}}^{\ell,p},(A-A^{t})\rangle\\
 & +\langle X,\log(DA^{t})\rangle-\langle\mathbf{1}^{\ell,p},DA^{t}\rangle
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
G(A,A^{t}) & =\sum_{\ell,p}\left[-\sum_{i}X_{\ell,p}\frac{D_{\ell,i}^{\prime}A_{i,p}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}^{t}}\left(\log\left(D_{\ell,i}^{\prime}A_{i,p}\right)-\log\left(\frac{D_{\ell,i}^{\prime}A_{i,p}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}^{t}}\right)\right)+\sum_{i}D_{\ell,i}^{\prime}A_{i,p}\right]\\
 & =\sum_{\ell,p}\left[X_{\ell,p}\sum_{i}w_{i}^{t,\ell,k}\log\left(\frac{w_{i}^{t,\ell,k}}{Z_{i}^{\ell,p}}\right)+\sum_{i}Z_{i}^{\ell,p}\right]
\end{align*}

\end_inset

where we have set set 
\begin_inset Formula $w_{i}^{t,\ell,k}=\frac{D_{\ell,i}^{\prime}A_{i,p}^{t}}{\sum_{k}D_{\ell,k}^{\prime}A_{k,p}^{t}}$
\end_inset

 and 
\begin_inset Formula $D_{\ell,i}^{\prime}A_{i,p}=Z_{i}^{\ell,p}$
\end_inset

.
 We have used
\begin_inset Formula 
\begin{align*}
-\sum_{i}Xw_{i}^{t}\left(\log\left(Z_{i}\right)-\log\left(w_{i}^{t}\right)\right) & =X\sum_{i}w_{i}^{t}\log\left(\frac{w_{i}^{t}}{Z_{i}}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Guillaume rederivation with Itakura Saito
\end_layout

\begin_layout Standard
The general loss is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{L}(A,D)=\sum_{\ell,p}\left[-X_{\ell,p}\log\left(\sum_{k}D_{\ell,k}^{\prime}A_{k,p}\right)+\sum_{k}D_{\ell,k}^{\prime}A_{k,p}\right]
\]

\end_inset

Let us focus on the function
\begin_inset Formula 
\begin{align*}
f(x,a,d) & :=-x\log\left(\sum_{i}a_{i}d_{i}\right)+\sum_{i}a_{i}d_{i}\\
 & =-x\log\left(\sum_{i}\frac{a_{i}d_{i}}{q_{i}}q_{i}\right)+\sum_{i}a_{i}d_{i}\\
 & \leq-x\sum_{i}q_{i}\log\left(\frac{a_{i}d_{i}}{q_{i}}\right)+\sum_{i}a_{i}d_{i}
\end{align*}

\end_inset

for 
\begin_inset Formula $\sum_{i}q_{i}=1$
\end_inset

, 
\begin_inset Formula $q_{i}>0$
\end_inset

.
 The last line follow from the Jensen inequality.
 Note that we have an equality for 
\begin_inset Formula 
\[
q_{i}=q_{i}^{*}=\frac{a_{i}d_{i}}{\sum_{j}a_{j}d_{j}}.
\]

\end_inset

This can be verified using
\begin_inset Formula 
\begin{align*}
\sum_{i}q_{i}^{*}\log\left(\frac{a_{i}d_{i}}{q_{i}^{*}}\right) & =\sum_{i}\frac{a_{i}d_{i}}{\sum_{j}a_{j}d_{j}}\log\left(\frac{a_{i}d_{i}}{\frac{a_{i}d_{i}}{\sum_{j}a_{j}d_{j}}}\right)\\
 & =\log\left(\sum_{j}a_{j}d_{j}\right)\frac{\sum_{i}a_{i}d_{i}}{\sum_{j}a_{j}d_{j}}\\
 & =\log\left(\sum_{j}a_{j}d_{j}\right).
\end{align*}

\end_inset

 Let us define
\begin_inset Formula 
\[
f_{J}(x,a,d,q):=-x\sum_{i}q_{i}\log\left(\frac{a_{i}d_{i}}{q_{i}}\right)+\sum_{i}a_{i}d_{i}
\]

\end_inset

Now les us write an auxiliary objective function
\begin_inset Formula 
\begin{align*}
\mathcal{L}_{J}(A,D,Q) & :=\sum_{\ell,p}\left[f_{J}(X_{\ell,p},A_{\cdot,p},D_{\ell,\cdot},Q_{\ell,p})\right]\\
 & =\sum_{\ell,p}\left[-X_{\ell,p}\sum_{k}Q_{\ell,p,k}\log\left(\frac{A_{k,p}D_{\ell,k}}{Q_{\ell,p,k}}\right)+\sum_{k}A_{k,p}D_{\ell,k}\right]\\
 & =-\sum_{p,k}\log\left(A_{k,p}\right)\sum_{\ell}X_{\ell,p}Q_{\ell,p,k}\\
 & -\sum_{\ell,k}\log\left(D_{\ell,k}\right)\sum_{p}X_{\ell,p}Q_{\ell,p,k}\\
 & +\sum_{\ell,p}\sum_{k}X_{\ell,p}Q_{\ell,p,k}\log\left(Q_{\ell,p,k}\right)\\
 & +\sum_{\ell,p}\sum_{k}A_{k,p}D_{\ell,k}
\end{align*}

\end_inset


\begin_inset Formula 
\[
\mathcal{L}(A,D)\leq\mathcal{L}_{J}(A,D,Q)
\]

\end_inset

Note that minimizing 
\begin_inset Formula $\mathcal{L}_{J}(A,D,Q)$
\end_inset

 with respect of 
\begin_inset Formula $A,D$
\end_inset

 is simple.
 (If we do not add further constraint, one can find a closed form solution.)
 The smooth regularizer is given by 
\begin_inset Formula 
\[
\frac{\lambda}{2}\text{tr}\left(A^{T}\Delta A\right)
\]

\end_inset


\begin_inset Formula 
\begin{align*}
\tilde{\psi_{t}}\left(A^{t},A\right) & =\frac{\lambda}{2}\text{tr}\left(A^{T}\Delta A\right)=\frac{\lambda}{2}\sum_{k}A_{k}^{T}\Delta A_{k}\\
 & =\frac{\lambda}{2}\sum_{k}A_{k}^{tT}\Delta A_{k}^{t}+\lambda\sum_{k}\left(A_{k}-A_{k}^{t}\right)^{T}\Delta A_{k}^{t}+\frac{\lambda}{2}\sum_{k}\left(A_{k}-A_{k}^{t}\right)^{T}\Delta\left(A_{k}-A_{k}^{t}\right)\\
 & \leq\frac{\lambda}{2}\sum_{k}A_{k}^{tT}\Delta A_{k}^{t}+\lambda\sum_{k}\left(A_{k}-A_{k}^{t}\right)^{T}\Delta A_{k}^{t}+\sigma(\Delta)\frac{\lambda}{2}\sum_{k}\|A_{k}-A_{k}^{t}\|^{2}
\end{align*}

\end_inset

I believe that minimizing with respect of 
\begin_inset Formula $Q$
\end_inset

 is simply doing 
\begin_inset Formula 
\[
\dot{Q_{\ell,p,i}}=\frac{A_{i,p}D_{\ell,i}}{\sum_{j}A_{j,p}D_{\ell,j}}.
\]

\end_inset

At optimimality of 
\begin_inset Formula $Q$
\end_inset

, we have
\begin_inset Formula 
\[
\mathcal{L}(A,D)=\mathcal{L}_{J}(A,D,\dot{Q})
\]

\end_inset


\begin_inset Formula 
\[
\nabla_{A}\mathcal{L}(A,D)=\nabla_{A}\mathcal{L}_{J}(A,D,\dot{Q})
\]

\end_inset


\begin_inset Formula 
\[
\nabla_{D}\mathcal{L}(A,D)=\nabla_{D}\mathcal{L}_{J}(A,D,\dot{Q})
\]

\end_inset


\end_layout

\begin_layout Subsection
Optimization
\end_layout

\begin_layout Standard
If we define 
\begin_inset Formula 
\[
\tilde{\phi}_{t,\gamma}\left(A^{t},A\right)=\phi\left(A^{t}\right)+\left\langle \nabla\phi\left(A^{t}\right),A-A^{t}\right\rangle +\gamma\left\Vert A-A^{t}\right\Vert _{F}^{2}
\]

\end_inset

Then we have
\begin_inset Formula 
\[
\phi\left(A^{t}\right)\le\tilde{\phi}_{t,\gamma}\left(A^{t},A\right)
\]

\end_inset


\end_layout

\begin_layout Standard
What is 
\begin_inset Formula $\gamma$
\end_inset

 here? The Lipschitz constant?
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\phi_{Q}\left(A,D\right)=\mathcal{L}_{J}\left(A,D,Q\right)$
\end_inset

, view as as a function of 
\begin_inset Formula $A,D$
\end_inset

.
 So we can minimize 
\begin_inset Formula $\mathcal{L}(A,D)$
\end_inset

 by solving a sequence of problem of the form
\begin_inset Formula 
\[
\min_{A,D}\lambda\psi\left(A\right)+\phi_{Q}\left(A,D\right)+\mathcal{I}\left(A\in\mathcal{S}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
So we can define a proximal step as
\begin_inset Formula 
\[
\min_{A}\tilde{\psi_{t}}\left(A^{t},A\right)+\tilde{\phi}_{Q}\left(A^{t},A,D\right)
\]

\end_inset


\end_layout

\begin_layout Standard
WE CAN DO LINESEARCH OVER GAMMA
\end_layout

\begin_layout Subsection
An algorithm?
\end_layout

\begin_layout Standard
Let us define our loss function as
\begin_inset Formula 
\begin{align*}
\mathcal{L}_{J}(A,D,Q) & :=-\sum_{p,k}\log\left(A_{k,p}\right)\sum_{\ell}X_{\ell,p}Q_{\ell,p,k}\\
 & -\sum_{\ell,k}\log\left(D_{\ell,k}\right)\sum_{p}X_{\ell,p}Q_{\ell,p,k}\\
 & +\sum_{\ell,p}\sum_{k}X_{\ell,p}Q_{\ell,p,k}\log\left(Q_{\ell,p,k}\right)\\
 & +\sum_{k,p}A_{k,p}\sum_{\ell}D_{\ell,k}\\
 & +\frac{\lambda}{2}\sum_{k,p}A_{k,p}\sum_{p_{2}}\Delta_{p,p_{2}}A_{k,p_{2}}
\end{align*}

\end_inset


\end_layout

\begin_layout Paragraph
A update: 
\end_layout

\begin_layout Quotation
The Lagrangian is given by 
\begin_inset Formula 
\[
\mathcal{D}_{J}(A,D,Q,\nu)=\mathcal{L}_{J}(A,D,Q)+\nu^{T}\left(A\mathbf{1}^{p}-\mathbf{1}^{k}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
A direct optimization with respec of 
\begin_inset Formula $A$
\end_inset

 is complicated because of the Laplacian.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left.\nabla_{A}\mathcal{D}_{J}(A,D^{t},Q)\right|_{k,p}=-\frac{\sum_{\ell}X_{\ell,p}Q_{\ell,p,k}^{t}}{A_{k,p}}+\sum_{i}D_{i,\ell}^{t}+\lambda\sum_{p_{2}}\Delta_{p,p_{2}}A_{k,p_{2}}+\nu_{p}=0
\]

\end_inset

So instead, we optimize an auxiliary function
\begin_inset Formula 
\begin{align*}
\tilde{\mathcal{D}_{J}}(A,A^{t},D^{t},Q^{t},\nu) & :=-\sum_{p,k}\log\left(A_{k,p}\right)\sum_{\ell}X_{\ell,p}Q_{\ell,p,k}-\sum_{\ell,k}\log\left(D_{\ell,k}\right)\sum_{p}X_{\ell,p}Q_{\ell,p,k}+\sum_{\ell,p}\sum_{k}X_{\ell,p}Q_{\ell,p,k}\log\left(Q_{\ell,p,k}\right)+\sum_{k,p}A_{k,p}\sum_{\ell}D_{\ell,k}\\
 & -\frac{\lambda}{2}\sum_{k}A_{k}^{tT}\Delta A_{k}^{t}+\lambda\sum_{k}A_{k}^{T}\Delta A_{k}^{t}+\sigma(\Delta)\frac{\lambda}{2}\|A-A^{t}\|_{F}^{2}\\
 & +\nu^{T}\left(A\mathbf{1}^{p}-\mathbf{1}^{k}\right)
\end{align*}

\end_inset

The gradient of the auxiliary function becomes
\begin_inset Formula 
\[
\left.\nabla_{A}\tilde{\mathcal{D}_{J}}(A,A^{t},D^{t},Q^{t},\nu)\right|_{k,p}=-\frac{\sum_{\ell}X_{\ell,p}Q_{\ell,p,k}^{t}}{A_{k,p}}+\sum_{\ell}D_{\ell,k}^{t}+\lambda\sum_{p_{2}}\Delta_{p,p_{2}}A_{k,p_{2}}^{t}+\lambda\sigma(\Delta)\left(A_{k,p}-A_{k,p}^{t}\right)+\nu_{k}=0
\]

\end_inset

Let us solve this equation assuming that 
\begin_inset Formula $A_{\ell,p}>0.$
\end_inset


\begin_inset Formula 
\[
\lambda\sigma(\Delta)A_{k,p}^{2}+A_{k,p}\left(\sum_{\ell}D_{\ell,k}^{t}+\lambda\sum_{p_{2}}\Delta_{p,p_{2}}A_{k,p_{2}}^{t}-A_{k,p}^{t}\lambda\sigma(\Delta)+\nu_{k}\right)-\sum_{\ell}X_{\ell,p}Q_{\ell,p,k}^{t}=0
\]

\end_inset

Let us define 
\begin_inset Formula $a=\lambda\sigma(\Delta)$
\end_inset

, 
\begin_inset Formula $b_{k,p}=\sum_{\ell}D_{\ell,k}^{t}+\lambda\sum_{p_{2}}\Delta_{p,p_{2}}A_{k,p_{2}}^{t}-A_{k,p}^{t}\lambda\sigma(\Delta)+\nu_{p}$
\end_inset

, and 
\begin_inset Formula $c_{k,p}=-\sum_{\ell}X_{\ell,p}Q_{\ell,p,k}^{t}$
\end_inset

.
 It is easy to see that 
\begin_inset Formula $a\geq0,c\leq0$
\end_inset

.
 So 
\begin_inset Formula $ac\leq0$
\end_inset

 and 
\begin_inset Formula 
\[
b^{2}-4ac\geq b^{2}\geq0
\]

\end_inset

Thus, we have two real solution, one negative and one positive.
 We are interested in the positive one:
\begin_inset Formula 
\[
A_{k,p}=\frac{-b+\sqrt{b^{2}-4ac}}{2a}
\]

\end_inset

Note that finding the right 
\begin_inset Formula $\nu$
\end_inset

 is probably harder as the function is not as nice as before.
 We have
\begin_inset Formula 
\begin{align*}
1 & =\sum_{p}A_{k,p}\\
 & =\sum_{p}\frac{-b_{k,p}\left(\nu_{k}\right)+\sqrt{b_{k,p}\left(\nu_{k}\right)^{2}-4ac_{k,p}}}{2a}\\
 & =\frac{1}{2a}\sum_{p}-s_{k,p}-\nu_{k}+\sqrt{\left(s_{k,p}+\nu_{k}\right)^{2}-4ac_{k,p}}
\end{align*}

\end_inset

where 
\begin_inset Formula $s_{p}=\sum_{\ell}D_{\ell,k}^{t}+\lambda\sum_{p_{2}}\Delta_{p,p_{2}}A_{k,p_{2}}^{t}-A_{k,p}^{t}\lambda\sigma(\Delta)$
\end_inset

.
 So we need to solve
\begin_inset Formula 
\[
f_{k}(\nu_{k})=n_{p}\nu_{k}+\sum_{p}s_{k,p}-\sum_{p}\sqrt{\left(s_{k,p}+\nu_{k}\right)^{2}-4ac_{k,p}}+2a=0
\]

\end_inset


\begin_inset Formula $f_{k}$
\end_inset

 is a (strictly) concave function.
 Furthermore for 
\begin_inset Formula 
\[
\lim_{\nu\rightarrow-\infty}f_{k}(\nu_{k})=-\infty
\]

\end_inset


\begin_inset Formula 
\[
\lim_{\nu\rightarrow\infty}f_{k}(\nu_{k})=2a
\]

\end_inset

Let us consider the function 
\begin_inset Formula $g(x)=x-\sqrt{x^{2}-d}.$
\end_inset

 
\end_layout

\begin_layout Standard
Check for a library for dicotomy...
 The challenge is that it is hard to solve multiple dicotomy problem in
 parallel.
 
\end_layout

\begin_layout Standard
Finally let us analyse 
\begin_inset Formula 
\begin{align*}
c_{k,p} & =-\sum_{\ell}X_{\ell,p}Q_{\ell,p,k}^{t}\\
 & =-\sum_{\ell}X_{\ell,p}\frac{A_{k,p}^{t}D_{\ell,k}^{t}}{\sum_{j}A_{j,p}^{t}D_{\ell,j}^{t}}\\
 & =-A_{k,p}^{t}\sum_{\ell}D_{\ell,k}^{t}\frac{X_{\ell,p}}{\sum_{j}A_{j,p}^{t}D_{\ell,j}^{t}}\\
 & =\left[-A^{t}\circ\left(D^{tT}\left(X\oslash D^{t}A^{t}\right)\right)\right]_{k,p}\\
 & =\left[-A^{t}\circ\left(P^{tT}G^{T}\left(X\oslash GP^{t}A^{t}\right)\right)\right]_{k,p}
\end{align*}

\end_inset


\end_layout

\begin_layout Paragraph
D update:
\end_layout

\begin_layout Standard
We simply that the gradient
\end_layout

\begin_layout Paragraph
\begin_inset Formula 
\[
\left.\nabla_{D}\mathcal{D}_{J}(A^{t},D,Q^{t})\right|_{\ell,p}=-\frac{\sum_{i}X_{\ell,i}Q_{\ell,i,p}^{t}}{D_{\ell,p}}+\sum_{p}A_{p,i}^{t}=0
\]

\end_inset


\end_layout

\begin_layout Standard
The update rule becomes
\begin_inset Formula 
\[
D_{\ell,p}=\frac{\sum_{i}X_{\ell,i}Q_{\ell,i,p}^{t}}{\sum_{p}A_{p,i}^{t}}
\]

\end_inset

Alternatively, we need to take into account that 
\begin_inset Formula $D=GP$
\end_inset

.
 So we find
\begin_inset Formula 
\begin{align*}
\left.\nabla_{P}\mathcal{D}_{J}(A,P,Q)\right|_{c_{0},k_{0}} & :=\left.\nabla_{P}\left(-\sum_{\ell,k}\log\left(D_{\ell,k}\right)\sum_{p}X_{\ell,p}Q_{\ell,p,k}+\sum_{\ell,p}\sum_{k}A_{k,p}D_{\ell,k}\right)_{D=GP}\right|_{c_{0},k_{0}}\\
 & =\left.\nabla_{P}\left(-\sum_{\ell,k,p}\log\left(\sum_{c}G_{\ell,c}P_{c,k}\right)X_{\ell,p}Q_{\ell,p,k}+\sum_{\ell,p,c,k}A_{k,p}G_{\ell,c}P_{c,k}\right)\right|_{c_{0},k_{0}}\\
 & =-\sum_{\ell}\frac{G_{\ell,c_{0}}\sum_{p}X_{\ell,p}Q_{\ell,p,k_{0}}}{\sum_{c}G_{\ell,c}P_{c,k_{0}}}+\sum_{p,\ell}A_{k_{0},p}G_{\ell,c_{0}}
\end{align*}

\end_inset

In matrix notation, we find
\begin_inset Formula 
\[
\nabla_{P}\mathcal{D}_{J}(A,P,Q)=-G^{T}\left(\left[XQ\right]\oslash GP\right)+G^{T}1^{\ell,p}A^{T}
\]

\end_inset

where 
\begin_inset Formula $\left[XQ\right]_{\ell,k}=\sum_{p}X_{\ell,p}Q_{\ell,p,k}.$
\end_inset

This is a bit annoying, because come back to a multiplicative algorithm!
 I believe we have to live with this...
\begin_inset Formula 
\begin{align*}
P^{t+1} & =P^{t}-\gamma_{t}\nabla_{P}\mathcal{D}_{J}(A,P,Q)\\
 & =P^{t}-\gamma_{t}\left[-G^{T}\left(\left[XQ\right]\oslash GP\right)+G^{T}1^{\ell,p}A^{T}\right]\\
 & =P^{t}+\gamma_{t}\left[G^{T}\left(\left[XQ\right]\oslash GP\right)-G^{T}1^{\ell,p}A^{T}\right]\\
 & =\left(P^{t}\oslash G^{T}1^{\ell,p}A^{T}\right)\circ\left(G^{T}\left(\left[XQ\right]\oslash GP^{t}\right)\right)
\end{align*}

\end_inset

where the learning rate is set to
\begin_inset Formula 
\[
\gamma_{t}=P^{t}\oslash G^{T}1^{\ell,p}A^{T}
\]

\end_inset


\end_layout

\begin_layout Standard
Let us replace Q with its current value with respect of 
\begin_inset Formula $P,A$
\end_inset

.
 We have
\begin_inset Formula 
\begin{align*}
\left[XQ\right]_{\ell,k} & =\sum_{p}X_{\ell,p}Q_{\ell,p,k}\\
 & =\sum_{p}X_{\ell,p}\frac{A_{k,p}D_{\ell,k}}{\sum_{j}A_{j,p}D_{\ell,j}}\\
 & =D_{\ell,k}\sum_{p}A_{k,p}\frac{X_{\ell,p}}{\sum_{j}A_{j,p}D_{\ell,j}}\\
 & =\left[D\circ\left(\left(X\oslash DA\right)A^{T}\right)\right]_{\ell,k}
\end{align*}

\end_inset

Given that 
\begin_inset Formula $D=GP$
\end_inset

, we find
\begin_inset Formula 
\begin{align*}
P^{t+1} & =\left(P^{t}\oslash G^{T}1^{\ell,p}A^{T}\right)\circ\left(G^{T}\left(\left[XQ\right]\oslash GP^{t}\right)\right)\\
 & =\left(P^{t}\oslash G^{T}1^{\ell,p}A^{T}\right)\circ\left(G^{T}\left(GP^{t}\circ\left(\left(X\oslash GP^{t}A\right)A^{T}\right)\oslash GP^{t}\right)\right)\\
 & =\left(P^{t}\oslash G^{T}1^{\ell,p}A^{T}\right)\circ\left(G^{T}\left(X\oslash GP^{t}A\right)A^{T}\right)
\end{align*}

\end_inset

which is the same update as we had previously!!!
\end_layout

\begin_layout Paragraph
Q update:
\end_layout

\begin_layout Standard
We simply do 
\begin_inset Formula 
\[
Q_{\ell,p,k}=\frac{A_{k,p}^{t}D_{\ell,k}^{t}}{\sum_{j}A_{j,p}^{t}D_{\ell,j}^{t}}.
\]

\end_inset


\end_layout

\begin_layout Paragraph
Dicotomy
\end_layout

\begin_layout Standard
We have the equation
\begin_inset Formula 
\[
f_{k}(\nu_{k})=\nu_{k}-\frac{1}{n_{p}}\sum_{p}\sqrt{\left(s_{k,p}+\nu_{k}\right)^{2}-4ac_{k,p}}+\frac{2a+\sum_{p}s_{k,p}}{n_{p}}
\]

\end_inset

Let us rewrite the function
\begin_inset Formula 
\[
f(\nu)=\nu-\frac{1}{n_{p}}\sum_{p}\sqrt{\left(s_{p}+\nu\right)^{2}-4ac_{p}}+d
\]

\end_inset

We bound 
\begin_inset Formula $f(\nu)$
\end_inset

 above and find
\begin_inset Formula 
\begin{align*}
f(\nu) & =\nu-\frac{1}{n_{p}}\sum_{p}\sqrt{\left(s_{p}+\nu\right)^{2}-4ac_{p}}+d\\
 & \leq\nu+d
\end{align*}

\end_inset

which implies that for 
\begin_inset Formula $\nu=-d$
\end_inset

 then 
\begin_inset Formula $f(\nu)\leq0.$
\end_inset

 Now let us bound it from below.
 To do so, we start by considering only one square root.
\begin_inset Formula 
\begin{align*}
f(\nu) & =\nu-\sqrt{\left(s_{p}+\nu\right)^{2}-4ac_{p}}+\frac{2a}{n_{p}}+s_{p}\geq0\\
\end{align*}

\end_inset

if 
\begin_inset Formula 
\begin{align*}
v+\frac{2a}{n_{p}}+s_{p} & \geq\sqrt{\left(s_{p}+\nu\right)^{2}-4ac_{p}}\\
 & \geq\sqrt{\left(s_{p}+\nu\right)^{2}-4ac_{p}}\\
\end{align*}

\end_inset

Let us write 
\begin_inset Formula $d=\frac{2a}{n_{p}}+s_{p},$
\end_inset

we find
\begin_inset Formula 
\[
\nu^{2}+2\nu d+d^{2}\geq\nu^{2}+s_{p}^{2}+2\nu s_{p}-4ac_{p}
\]

\end_inset

which leads to
\begin_inset Formula 
\begin{align*}
\nu & \geq\frac{s_{p}^{2}-4ac_{p}+d^{2}}{d-s_{p}}\\
 & =\frac{s_{p}^{2}-4ac_{p}+4a^{2}+4as_{p}+s_{p}^{2}}{\frac{2a}{n_{p}}}\\
 & =n_{p}\left(\frac{s_{p}^{2}}{a}+2a+2\left(s_{p}-c_{p}\right)\right)
\end{align*}

\end_inset

At this point, we need to consider the worse case which give us a global
 bound
\begin_inset Formula 
\[
\nu\geq n_{p}\max_{p}\left[\frac{s_{p}^{2}}{a}+2a+2\left(s_{p}-c_{p}\right)\right]
\]

\end_inset


\end_layout

\begin_layout Subsection
Definitions and stuff to know
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $f$
\end_inset

 be a differentiable and convex function.
 Then we have
\begin_inset Formula 
\[
f(x)=f(x_{t})+\left\langle \nabla f(x_{t}),x-x_{t}\right\rangle +D_{f}\left(x\|x_{t}\right)
\]

\end_inset

where 
\begin_inset Formula $D_{f}$
\end_inset

 is the Bregman divergence associated with 
\begin_inset Formula $f$
\end_inset

.
 This follows from the definition of the Bregmann divergence:
\begin_inset Formula 
\[
D_{f}\left(x\|x_{t}\right):=f(x)-f(x_{t})-\left\langle \nabla f(x_{t}),x-x_{t}\right\rangle .
\]

\end_inset

For example if 
\begin_inset Formula $f(x)=-\log(x),$
\end_inset

 then 
\begin_inset Formula 
\[
D_{f}\left(x\|x_{t}\right):=-\log\left(\frac{x}{x_{t}}\right)+\frac{x}{x_{t}}-1,
\]

\end_inset

which is the ItakuraSaito distance.
\end_layout

\begin_layout Subsection
A connection here
\end_layout

\begin_layout Standard
Question: can we just use 
\begin_inset Formula $\dot{Q}$
\end_inset

 and assume that it is equal to 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathcal{L}$
\end_inset

?
\end_layout

\begin_layout LyX-Code
\begin_inset Formula 
\[
\mathcal{L}_{J}(A,D^{t},Q^{t})=\mathcal{L}_{J}(A^{t},D^{t},Q^{t})+\left\langle \nabla_{A}\mathcal{L}_{J}(A^{t},D^{t},Q^{t}),A-A^{t}\right\rangle +D_{\mathcal{L}_{J}}\left(A\|A^{t}\right)
\]

\end_inset


\begin_inset Formula 
\begin{align*}
\left\langle \nabla_{A}\mathcal{L}_{J}(A^{t},D^{t},\dot{Q^{t}}),A-A^{t}\right\rangle  & =\sum_{\ell,p}\left(\sum_{i}D_{i,\ell}^{t}-\frac{\sum_{i}X_{i,p}\dot{Q_{i,p,\ell}^{t}}}{A_{\ell,p}^{t}}\right)\left(A_{\ell,p}-A_{\ell,p}^{t}\right)\\
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Taking the bremstrahlung out of G
\end_layout

\begin_layout Standard
Let us consider the loss
\begin_inset Formula 
\begin{align*}
L(P,B) & =-\left\langle X,\log\left(\left(GP+B\right)A\right)\right\rangle +\left\langle \mathbf{1}^{\ell,p},\left(GP+B\right)A\right\rangle \\
 & =-\left\langle X,\log\left(\left(GP+B\right)A\right)\right\rangle +\left\langle \mathbf{1}^{\ell,p},GPA\right\rangle +\left\langle \mathbf{1}^{\ell,p},BA\right\rangle 
\end{align*}

\end_inset

The gradient with respect of 
\begin_inset Formula $P$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 are
\begin_inset Formula 
\[
\nabla_{P}\mathcal{L}=-G^{T}\left(X\oslash\left(\left(GP+B\right)A\right)\right)A^{T}+G^{T}\mathbf{1}^{\ell,p}A^{T}
\]

\end_inset


\begin_inset Formula 
\[
\nabla_{B}\mathcal{L}=-\left(X\oslash\left(\left(GP+B\right)A\right)\right)A^{T}+\mathbf{1}^{\ell,p}A^{T}
\]

\end_inset

Now let 
\begin_inset Formula $B=G_{2}P_{2}=\left(\begin{array}{ccc}
\frac{1}{\mathbf{e}} & \mathbf{1}^{\ell} & \mathbf{e}\end{array}\right)P_{2}$
\end_inset

 where 
\begin_inset Formula $G_{2}\in\mathbb{R}^{\ell\times3}$
\end_inset

 and 
\begin_inset Formula $P_{2}\in\mathbb{R}^{3\times c}$
\end_inset

.
\begin_inset Formula 
\[
\nabla_{P_{2}}\mathcal{L}=-G_{2}^{T}\left(X\oslash\left(\left(GP+G_{2}P_{2}\right)A\right)\right)A^{T}+G_{2}^{T}\mathbf{1}^{\ell,p}A^{T}
\]

\end_inset

To update 
\begin_inset Formula $P$
\end_inset

, we use an multiplicative step (almost no change)
\begin_inset Formula 
\begin{align*}
P_{t+1} & =P_{t}-\gamma\circ\nabla_{P}\mathcal{L}\vert_{A_{t},P_{t}}\\
 & =P_{t}-\gamma\circ\left(-G^{T}\left(X\oslash\left(\left(GP+B\right)A\right)\right)A^{T}+G^{T}\mathbf{1}^{\ell,p}A^{T}\right)\\
 & =\left(P_{t}\oslash G^{T}\mathbf{1}^{\ell,p}A^{T}\right)\circ\left(G^{T}\left(X\oslash\left(\left(GP+B\right)A\right)\right)A^{T}\right)
\end{align*}

\end_inset

For 
\begin_inset Formula $P_{2,t}$
\end_inset

, 
\begin_inset Formula $\gamma$
\end_inset

 is constant.
 What should be its value?
\begin_inset Formula 
\begin{align*}
P_{2,t+1} & =P_{2,t}-\gamma\circ\nabla_{P_{2}}\mathcal{L}\vert_{A_{t},P_{t}}\\
 & =P_{2,t}-\gamma\circ\left(-G_{2}^{T}\left(X\oslash\left(\left(GP+G_{2}P_{2}\right)A\right)\right)A^{T}+G_{2}^{T}\mathbf{1}^{\ell,p}A^{T}\right)\\
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Projection onto the convex set
\end_layout

\begin_layout Standard
Let us consider the convex set 
\begin_inset Formula 
\[
\mathcal{S}=\left\{ \left(a,b,c\right)\in\mathbb{R}^{3}|a\geq0,c\geq0,b^{2}\leq4ac\right\} 
\]

\end_inset


\end_layout

\begin_layout Subsection
Convexity
\end_layout

\begin_layout Standard
Let us verify the convexity of the set.
 Given
\begin_inset Formula 
\[
\left(a_{1},b_{1},c_{1}\right),\left(a_{2},b_{2},c_{2}\right)\in\mathcal{S},
\]

\end_inset

we see that 
\begin_inset Formula $\left(\frac{a_{1}+a_{2}}{2},\frac{b_{1}+b_{2}}{2},\frac{c_{1}+c_{2}}{2}\right)\in\mathcal{S}.$
\end_inset

 It is trivial to see that 
\begin_inset Formula $\frac{a_{1}+a_{2}}{2}\ge0$
\end_inset

 and 
\begin_inset Formula $\frac{c_{1}+c_{2}}{2}\geq0$
\end_inset

.
 let us check that 
\begin_inset Formula 
\[
\left(\frac{b_{1}+b_{2}}{2}\right)^{2}\le4\frac{\left(a_{1}+a_{2}\right)}{2}\frac{\left(c_{1}+c_{2}\right)}{2}=\left(a_{1}+a_{2}\right)\left(c_{1}+c_{2}\right)=a_{1}c_{1}+a_{2}c_{2}+a_{1}c_{2}+a_{2}c_{1}.
\]

\end_inset

If we prove the case for 
\begin_inset Formula $b_{1},b_{2}\geq0$
\end_inset

, then all other case are trivially proved.
 So let us assume 
\begin_inset Formula $b_{1},b_{2}\geq0$
\end_inset

.
 We have
\begin_inset Formula 
\begin{align*}
\left(\frac{b_{1}+b_{2}}{2}\right)^{2} & =\frac{1}{4}\left(b_{1}^{2}+b_{2}^{2}+2b_{1}b_{2}\right)\\
 & \leq a_{1}c_{1}+a_{2}c_{2}+2\sqrt{a_{1}c_{1}a_{2}c_{2}}\\
 & \leq a_{1}c_{1}+a_{2}c_{2}+a_{1}c_{2}+a_{2}c_{1}
\end{align*}

\end_inset

The last line follow from the AMGM inequality.
 
\begin_inset Formula 
\[
\sqrt{a_{1}c_{2}\cdot a_{2}c_{1}}\leq\frac{a_{1}c_{2}+a_{2}c_{1}}{2}.
\]

\end_inset


\end_layout

\begin_layout Subsection
Projection
\end_layout

\begin_layout Standard
Let us consider the following problem
\begin_inset Formula 
\[
\dot{x}=\arg\min_{x\in\mathcal{S}}\frac{1}{2}\left\Vert x-x_{0}\right\Vert ^{2}.
\]

\end_inset


\end_layout

\begin_layout Subsection
General case
\end_layout

\begin_layout Standard
The lagrangian is written
\begin_inset Formula 
\[
\mathcal{L}(a,b,c,\lambda)=\frac{1}{2}\left\Vert \left(\begin{array}{c}
a_{0}\\
b_{0}\\
c_{0}
\end{array}\right)-\left(\begin{array}{c}
a\\
b\\
c
\end{array}\right)\right\Vert ^{2}+\lambda\left(b^{2}-4ac\right)
\]

\end_inset

The KKT conditions are
\begin_inset Formula 
\[
\nabla\mathcal{L}=0,
\]

\end_inset

i.e.
 
\begin_inset Formula 
\[
\left(a-a_{0}\right)-\lambda4c=0
\]

\end_inset


\begin_inset Formula 
\[
\left(c-c_{0}\right)-\lambda4a=0
\]

\end_inset


\begin_inset Formula 
\[
\left(b-b_{0}\right)+2\lambda b=0
\]

\end_inset


\begin_inset Formula 
\[
\lambda\left(b^{2}-4ac\right)=0
\]

\end_inset


\begin_inset Formula 
\[
\lambda\geq0
\]

\end_inset


\begin_inset Formula 
\[
\left(b^{2}-4ac\right)\leq0
\]

\end_inset

Let us consider the case where 
\begin_inset Formula $b_{0}^{2}-4a_{0}c_{0}\geq0$
\end_inset

, i.e.
 when the constraint is not satisfied.
 In that case, we know that 
\begin_inset Formula $\left(b^{2}-4ac\right)=0$
\end_inset

, i.e.
\begin_inset Formula 
\[
a=\frac{b^{2}}{4c},c=\frac{b^{2}}{4a}.
\]

\end_inset

We find 
\begin_inset Formula $\left(a-a_{0}\right)-\lambda\frac{b^{2}}{a}=0$
\end_inset

 and 
\begin_inset Formula $\left(c-c_{0}\right)-\lambda\frac{b^{2}}{c}=0.$
\end_inset

 Thus 
\begin_inset Formula 
\[
a^{2}-aa_{0}-\lambda b^{2}=0
\]

\end_inset

and 
\begin_inset Formula $a=\frac{a_{0}+\sqrt{a_{0}^{2}+4\lambda b^{2}}}{2},$
\end_inset

 
\begin_inset Formula $c=\frac{c_{0}+\sqrt{c_{0}^{2}+4\lambda b^{2}}}{2}.$
\end_inset

(We discard the other solution as we know that 
\begin_inset Formula $a,c\geq$
\end_inset

0).
 Now we know from 
\begin_inset Formula $\left(b-b_{0}\right)+2\lambda b=0$
\end_inset

 that 
\begin_inset Formula 
\[
\lambda=\frac{b_{0}-b}{2b}
\]

\end_inset

We observe that if 
\begin_inset Formula $b_{0}\geq0$
\end_inset

, 
\begin_inset Formula $b\leq b_{0}$
\end_inset

 as 
\begin_inset Formula $\lambda\ge0.$
\end_inset

 If 
\begin_inset Formula $b_{0}\leq0,$
\end_inset

 then 
\begin_inset Formula $b\geq b_{0}.$
\end_inset

 Now we have 
\begin_inset Formula $4\lambda b^{2}=4\frac{b_{0}-b}{2b}b^{2}=2b_{0}b-2b^{2}$
\end_inset


\begin_inset Formula 
\begin{align*}
b^{2} & =4ac\\
 & =\left(a_{0}+\sqrt{a_{0}^{2}+2b_{0}b-2b^{2}}\right)\left(c_{0}+\sqrt{c_{0}^{2}+2b_{0}b-2b^{2}}\right)\\
 & =a_{0}c_{0}+c_{0}\sqrt{a_{0}^{2}+2b_{0}b-b^{2}}+a_{0}\sqrt{c_{0}^{2}+2b_{0}b-b^{2}}+\sqrt{a_{0}^{2}+2b_{0}b-b^{2}}\sqrt{c_{0}^{2}+2b_{0}b-b^{2}}\\
 & =a_{0}c_{0}+c_{0}\sqrt{a_{0}^{2}+2b_{0}b-b^{2}}+a_{0}\sqrt{c_{0}^{2}+2b_{0}b-b^{2}}+\sqrt{\left(2b_{0}b-b^{2}\right)^{2}+\left(2b_{0}b-b^{2}\right)\left(c_{0}^{2}+a_{0}^{2}\right)+\left(c_{0}^{2}a_{0}^{2}\right)}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Case 
\begin_inset Formula $a=1$
\end_inset


\end_layout

\begin_layout Standard
The lagrangian is written
\begin_inset Formula 
\[
\mathcal{L}(a,b,c,\lambda)=\frac{1}{2}\left\Vert \left(\begin{array}{c}
c\\
b
\end{array}\right)-\left(\begin{array}{c}
c_{0}\\
b_{0}
\end{array}\right)\right\Vert ^{2}+\lambda\left(b^{2}-4c\right)
\]

\end_inset

And the KKT conditions are
\begin_inset Formula 
\[
\left(c-c_{0}\right)-\lambda4=0
\]

\end_inset


\begin_inset Formula 
\[
\left(b-b_{0}\right)+2\lambda b=0
\]

\end_inset


\begin_inset Formula 
\[
\lambda\left(b^{2}-4c\right)=0
\]

\end_inset


\begin_inset Formula 
\[
\lambda\geq0
\]

\end_inset


\begin_inset Formula 
\[
\left(b^{2}-4c\right)\leq0
\]

\end_inset

Let us assume that the constraint is not satisfied.
 Then we know that 
\begin_inset Formula $\left(b^{2}-4c\right)=0$
\end_inset

.
 As a result, 
\begin_inset Formula $c=\frac{b^{2}}{4}$
\end_inset

.
 Using the fact that 
\begin_inset Formula $\frac{c-c_{0}}{4}=\lambda$
\end_inset

, we obtain
\begin_inset Formula 
\[
b^{3}+b\left(8-4c_{0}\right)-8b_{0}=0
\]

\end_inset

which is a depressed cubic equation.
 The discriminant is
\begin_inset Formula 
\[
\Delta=4\left(8-4c_{0}\right)^{3}+27\left(8b_{0}\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Other parametrization
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
b_{0}\frac{\left(E_{0}-E\right)}{E}+b_{1}\frac{\left(E_{0}-E\right)^{2}}{E} & =\frac{1}{E}\left[b_{0}\left(E_{0}-E\right)+b_{1}\left(E_{0}-E\right)^{2}\right]\\
 & =\frac{1}{E}\left[b_{1}E^{2}-E\left(b_{0}+2b_{1}E_{0}\right)+\left(b_{0}E_{0}+b_{1}E_{0}^{2}\right)\right]\\
\end{align*}

\end_inset

We need that 
\begin_inset Formula $b_{1}\geq0$
\end_inset

 and that 
\begin_inset Formula 
\begin{align*}
\Delta & =\left(b_{0}+2b_{1}E_{0}\right)^{2}-4b_{1}\left(b_{0}E_{0}+b_{1}E_{0}^{2}\right)\\
 & =b_{0}^{2}+4E_{0}b_{0}b_{1}+4E_{0}^{2}b_{1}^{2}-4b_{1}b_{0}E_{0}-4b_{1}^{2}E_{0}^{2}\\
 & =b_{0}^{2}\leq0\\
\end{align*}

\end_inset

Let us rewrite the expression and assume that 
\begin_inset Formula $0\leq E\leq E_{0}.$
\end_inset

 We need that 
\begin_inset Formula $b_{0}\left(E_{0}-E\right)+b_{1}\left(E_{0}-E\right)^{2}\geq0$
\end_inset

 implies with a simple change of variable that 
\begin_inset Formula 
\[
b_{0}E+b_{1}E^{2}\geq0\hspace{1em}\text{for }0\leq E\leq E_{0}.
\]

\end_inset

So we need 
\begin_inset Formula $b_{0}+b_{1}E\geq0$
\end_inset

 for 
\begin_inset Formula $E=0$
\end_inset

 and 
\begin_inset Formula $E=E_{0}.$
\end_inset

 As a result, we need 
\begin_inset Formula 
\[
b_{0}\geq0,\hspace{1em}b_{1}\geq-\frac{b_{0}}{E_{0}}
\]

\end_inset

Let us search for the projection operator
\begin_inset Formula 
\[
\mathcal{L}(a,b,c,\lambda)=\frac{1}{2}\left\Vert \left(\begin{array}{c}
a\\
b
\end{array}\right)-\left(\begin{array}{c}
a_{0}\\
b_{0}
\end{array}\right)\right\Vert ^{2}-\lambda\left(a+bE_{0}\right)
\]

\end_inset

The KKT conditions are
\begin_inset Formula 
\[
\left(a-a_{0}\right)-\lambda=0
\]

\end_inset


\begin_inset Formula 
\[
\left(b-b_{0}\right)-\lambda E_{0}=0
\]

\end_inset


\begin_inset Formula 
\[
\lambda\left(a+bE_{0}\right)=0
\]

\end_inset


\begin_inset Formula 
\[
\lambda\geq0
\]

\end_inset


\begin_inset Formula 
\[
a+bE_{0}\ge0
\]

\end_inset

We assume that 
\begin_inset Formula $a_{0}\geq0.$
\end_inset

 If 
\begin_inset Formula $a+bE_{0}\ge0$
\end_inset

, then 
\begin_inset Formula $\lambda=0$
\end_inset

 and 
\begin_inset Formula $a=a_{0},b=b_{0}.$
\end_inset

 If 
\begin_inset Formula $a_{0}+b_{0}E_{0}\leq0$
\end_inset

, we have 
\begin_inset Formula $a=-bE_{0}$
\end_inset

 .
 So 
\begin_inset Formula $-\left(bE_{0}+a_{0}\right)=\lambda$
\end_inset

 and 
\begin_inset Formula $\left(b-b_{0}\right)+\left(bE_{0}+a_{0}\right)E_{0}=0$
\end_inset

.
 As a result, we find
\begin_inset Formula 
\[
b=\frac{b_{0}-a_{0}E_{0}}{\left(1+E_{0}^{2}\right)}
\]

\end_inset


\begin_inset Formula 
\[
\lambda=-\left(\frac{b_{0}-a_{0}E_{0}}{\left(1+E_{0}^{2}\right)}E_{0}+a_{0}\right)
\]

\end_inset


\begin_inset Formula 
\[
a=a_{0}-\frac{a_{0}+b_{0}E_{0}}{\left(1+E_{0}^{2}\right)}
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Using a different normalization
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
b_{0}\frac{\left(E_{0}-E\right)}{E}+b_{1}\frac{\left(E_{0}-E\right)^{2}}{E_{0}E} & =\frac{1}{E}\left[b_{0}\left(E_{0}-E\right)+b_{1}\frac{\left(E_{0}-E\right)^{2}}{E_{0}}\right]\\
 & =\frac{1}{E}\left[b_{1}\frac{E^{2}}{E_{0}}-E\left(b_{0}+2b_{1}\right)+\left(b_{0}E_{0}+b_{1}E_{0}\right)\right]\\
\end{align*}

\end_inset

Let us rewrite the expression and assume that 
\begin_inset Formula $0\leq E\leq E_{0}.$
\end_inset

 We need that 
\begin_inset Formula $b_{0}\left(E_{0}-E\right)+b_{1}\frac{\left(E_{0}-E\right)^{2}}{E_{0}}\geq0$
\end_inset

 implies with a simple change of variable that 
\begin_inset Formula 
\[
b_{0}E^{\prime}E_{0}+b_{1}E^{\prime}{}^{2}\geq0\hspace{1em}\text{for }0\leq E^{\prime}\leq E_{0}.
\]

\end_inset

So we need 
\begin_inset Formula $b_{0}E_{0}+b_{1}E\geq0$
\end_inset

 for 
\begin_inset Formula $E=0$
\end_inset

 and 
\begin_inset Formula $E=E_{0}.$
\end_inset

 As a result, we need 
\begin_inset Formula 
\[
b_{0}\geq0,\hspace{1em}b_{1}\geq-b_{0}
\]

\end_inset


\end_layout

\begin_layout Subsection
Using an auxiliary function (not working)
\end_layout

\begin_layout Standard
The lagrangian is given by
\begin_inset Formula 
\begin{align*}
\mathcal{L}(A,P) & ={\color{magenta}-\langle X,\log(GPA)\rangle+\langle\mathbf{1}^{\ell,p},GPA\rangle}{\color{green}-\lambda\left(b_{0}+b_{1}E_{0}\right)}\\
 & ={\color{magenta}-\sum_{\ell,p}X_{\ell,p}\log\left(\sum_{c,k}G_{\ell,k}A_{c,p}P_{c,k}\right)+\sum_{\ell,p,c,k}G_{\ell,k}A_{c,p}P_{c,k}}{\color{green}-\sum_{c}\lambda_{c}\left(b_{0,c}+b_{1,c}E_{0}\right)}
\end{align*}

\end_inset

The auxiliary function becomes
\begin_inset Formula 
\begin{align*}
G(P,P^{t}) & ={\color{magenta}\sum_{\ell,p,c,k}G_{\ell,k}A_{c,p}P_{c,k}}{\color{green}-\sum_{c}\lambda_{c}\left(b_{0,c}+b_{1,c}E_{0}\right)}\\
 & {\color{purple}-\sum_{\ell,p,i,j}X_{\ell,p}\frac{G_{\ell,j}A_{i,p}P_{i,j}^{t}}{\sum_{c,k}G_{\ell,k}A_{c,p}P_{c,k}^{t}}\left(\log\left(G_{\ell,j}A_{i,p}P_{i,j}\right)-\log\left(\frac{G_{\ell,j}A_{i,p}P_{i,j}^{t}}{\sum_{c,k}G_{\ell,k}A_{c,p}P_{c,k}^{t}}\right)\right)}
\end{align*}

\end_inset


\series bold
\color red
This last step is not allowed as we have the log of potential negative terms!
 
\series default
\color inherit
Here 
\begin_inset Formula $b_{0,c},b_{1,c}=P_{c,1},P_{c,2}$
\end_inset

.
 Now let us take the gradient of the auxiliary function 
\begin_inset Formula 
\begin{align*}
\left.\nabla_{P}G(P,P^{t})\right|_{c_{0},k_{0}} & ={\color{magenta}\sum_{\ell,p}G_{\ell,k_{0}}A_{c_{0},p}}{\color{green}-\lambda_{c_{0}}\left(\delta_{k_{0}=1}+\delta_{k_{0}=2}E_{0}\right)}\\
 & {\color{purple}-\frac{1}{P_{c_{0},k_{0}}}\sum_{\ell,p}X_{\ell,p}\frac{G_{\ell,k_{0}}A_{c_{0},p}P_{c_{0},k_{0}}^{t}}{\sum_{c,k}G_{\ell,k}A_{c,p}P_{c,k}^{t}}}\\
 & =0
\end{align*}

\end_inset

Here 
\begin_inset Formula $\delta_{k_{0}=i}=\begin{cases}
0 & \text{if }k_{0}\neq i\\
1 & \text{if }k_{0}=i.
\end{cases}$
\end_inset

 As a result we find
\begin_inset Formula 
\begin{align*}
P_{c_{0},k_{0}} & =P_{c_{0},k_{0}}^{t}\frac{\sum_{\ell,p}G_{\ell,c_{0}}\frac{X_{\ell,p}}{\sum_{c,k}G_{\ell,k}A_{c,p}P_{c,k}^{t}}A_{k_{0},p}}{\sum_{\ell,p}G_{\ell,k_{0}}A_{c_{0},p}{\color{green}-\lambda_{c_{0}}\left(\delta_{k_{0}=1}+\delta_{k_{0}=2}E_{0}\right)}}\\
 & {\color{purple}}\\
\end{align*}

\end_inset

If the previous iteration satisfies the constraint, we have 
\begin_inset Formula $\sum_{c,k}G_{\ell,k}A_{c,p}P_{c,k}^{t}>0$
\end_inset

.
 As a result, for 
\begin_inset Formula $k_{0}>2$
\end_inset

, nothing is changed from the previous algorithm.
 
\end_layout

\begin_layout Standard
Let us now care about the case 
\begin_inset Formula $k_{0}=1,2$
\end_inset

.
 If the constraint is satisfied for 
\begin_inset Formula $c_{0}$
\end_inset

, i.e., if 
\begin_inset Formula 
\[
\frac{\sum_{\ell,p}X_{\ell,p}\frac{G_{\ell,1}A_{c_{0},p}P_{c_{0},1}^{t}}{\sum_{c,k}G_{\ell,c}A_{k,p}P_{c,k}^{t}}}{\sum_{\ell,p}G_{\ell,c_{0}}A_{k_{0},p}}+\frac{\sum_{\ell,p}X_{\ell,p}\frac{G_{\ell,1}A_{c_{0},p}P_{c_{0},2}^{t}}{\sum_{c,k}G_{\ell,c}A_{k,p}P_{c,k}^{t}}}{\sum_{\ell,p}G_{\ell,c_{0}}A_{k_{0},p}}E_{0}\geq0
\]

\end_inset

, then 
\begin_inset Formula $\lambda_{c_{0}}=0$
\end_inset

 and nothing in green matters.
 Otherwise we know that the constraint need to be satisfied, i.e.
\begin_inset Formula 
\[
\frac{\sum_{\ell,p}X_{\ell,p}\frac{G_{\ell,1}A_{c_{0},p}P_{c_{0},1}^{t}}{\sum_{c,k}G_{\ell,c}A_{k,p}P_{c,k}^{t}}}{\sum_{\ell,p}G_{\ell,c_{0}}A_{k_{0},p}-\lambda_{c_{0}}E_{0}}+\frac{\sum_{\ell,p}X_{\ell,p}\frac{G_{\ell,1}A_{c_{0},p}P_{c_{0},2}^{t}}{\sum_{c,k}G_{\ell,c}A_{k,p}P_{c,k}^{t}}}{\sum_{\ell,p}G_{\ell,c_{0}}A_{k_{0},p}-\lambda_{c_{0}}E_{0}}E_{0}=0
\]

\end_inset

As a result, we have to solve:
\begin_inset Formula 
\[
\frac{n_{1}}{d_{1}-\lambda}+\frac{n_{2}}{d_{2}-\lambda}=0.
\]

\end_inset

with 
\begin_inset Formula $n_{1},d_{1},d_{2}\geq0,$
\end_inset

 
\begin_inset Formula $n_{2}\leq0$
\end_inset

.
 Obviously, 
\begin_inset Formula $0\leq\lambda\leq d_{1}.$
\end_inset

 We find
\begin_inset Formula 
\[
\lambda=\frac{n_{1}d_{2}+n_{2}d_{1}}{n_{1}+n_{2}}.
\]

\end_inset


\end_layout

\begin_layout Subsection
Different parametrization
\end_layout

\begin_layout Standard
Let us use a different, yet equivalent parametrization that has the property
 to always have positive terms inside the accepted domain.
 As a reminder, we have:
\begin_inset Formula 
\[
b_{0}\frac{\left(E_{0}-E\right)}{E}+b_{1}\frac{\left(E_{0}-E\right)^{2}}{E_{0}E}\geq0\hspace{1em}\forall0<E\leq E_{0}
\]

\end_inset

which implies
\begin_inset Formula 
\[
b_{0}\geq0,\hspace{1em}b_{0}+b_{1}\geq0.
\]

\end_inset

Let us consider the parametrization
\begin_inset Formula 
\[
b_{1}^{\prime}=b_{0}+b_{1}\geq0
\]

\end_inset

 and 
\begin_inset Formula $b_{0}^{\prime}=b_{0}\geq0.$
\end_inset

 We find that 
\begin_inset Formula $b_{1}=b_{1}^{\prime}-b_{0}^{\prime}.$
\end_inset

 The new polynome becomes
\begin_inset Formula 
\[
b_{0}^{\prime}\frac{\left(E_{0}-E\right)}{E}+\left(b_{1}^{\prime}-b_{0}^{\prime}\right)\frac{\left(E_{0}-E\right)^{2}}{E_{0}E}=b_{0}^{\prime}\left[\frac{\left(E_{0}-E\right)}{E}-\frac{\left(E_{0}-E\right)^{2}}{E_{0}E}\right]+b_{1}^{\prime}\frac{\left(E_{0}-E\right)^{2}}{E_{0}E}
\]

\end_inset


\begin_inset Formula 
\begin{align*}
\frac{\left(E_{0}-E\right)}{E}-\frac{\left(E_{0}-E\right)^{2}}{E_{0}E} & =\frac{1}{E}\left(E-\frac{E^{2}}{E_{0}}\right)\\
 & =1-\frac{E}{E_{0}}
\end{align*}

\end_inset


\end_layout

\end_body
\end_document
